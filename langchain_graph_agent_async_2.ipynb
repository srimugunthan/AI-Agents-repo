{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uToE66ptNA81"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install aiohttp -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dH5H64_1NFJj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import aiohttp\n",
        "import asyncio\n",
        "import json\n",
        "from typing import Any, List, Optional, TypedDict\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.language_models.llms import BaseLLM\n",
        "from langchain_core.outputs import LLMResult\n",
        "from langchain_core.callbacks.manager import AsyncCallbackManagerForLLMRun, CallbackManagerForLLMRun\n",
        "from pydantic import Field\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "# LangGraph Imports\n",
        "from langgraph.graph import StateGraph, END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbdwAvlALpCs"
      },
      "outputs": [],
      "source": [
        "# --- 1. Async Custom LLM Implementation ---\n",
        "\n",
        "class AsyncCustomHTTPGemini(BaseLLM):\n",
        "    \"\"\"\n",
        "    An async custom LangChain LLM wrapper that interacts with the Google Gemini API\n",
        "    using aiohttp for asynchronous HTTP requests.\n",
        "    \"\"\"\n",
        "\n",
        "    # Model and API Configuration\n",
        "    api_key: Optional[str] = None\n",
        "    model_name: str = Field(default=\"gemini-2.5-flash\", alias=\"model\")\n",
        "    base_url: str = \"https://generativelanguage.googleapis.com/v1beta/models/\"\n",
        "    response_schema: Optional[dict] = None\n",
        "    _session: Optional[aiohttp.ClientSession] = None\n",
        "\n",
        "    class Config:\n",
        "        arbitrary_types_allowed = True\n",
        "\n",
        "    def __init__(self, **kwargs: Any):\n",
        "        super().__init__(**kwargs)\n",
        "        if not self.api_key:\n",
        "            self.api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "        if not self.api_key:\n",
        "            raise ValueError(\"GEMINI_API_KEY must be provided or set as an environment variable.\")\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"Return type of LLM.\"\"\"\n",
        "        return \"async_custom_http_gemini\"\n",
        "\n",
        "    async def _get_session(self) -> aiohttp.ClientSession:\n",
        "        \"\"\"Get or create an aiohttp session.\"\"\"\n",
        "        if self._session is None or self._session.closed:\n",
        "            self._session = aiohttp.ClientSession()\n",
        "        return self._session\n",
        "\n",
        "    async def _acall(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Async HTTP POST request to the Gemini API.\n",
        "        \"\"\"\n",
        "        # 1. Construct the API Endpoint\n",
        "        api_endpoint = f\"{self.base_url}{self.model_name}:generateContent\"\n",
        "        url = f\"{api_endpoint}?key={self.api_key}\"\n",
        "        headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "        # 2. Construct the request body\n",
        "        request_data = {\n",
        "            \"contents\": [\n",
        "                {\n",
        "                    \"parts\": [\n",
        "                        {\"text\": prompt}\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        # 3. Add generation config for JSON output if schema is present\n",
        "        if self.response_schema:\n",
        "            request_data[\"generationConfig\"] = {\n",
        "                \"responseMimeType\": \"application/json\",\n",
        "                \"responseSchema\": self.response_schema\n",
        "            }\n",
        "\n",
        "        # 4. Send async request\n",
        "        try:\n",
        "            session = await self._get_session()\n",
        "            async with session.post(url, headers=headers, json=request_data) as response:\n",
        "                response.raise_for_status()\n",
        "                response_json = await response.json()\n",
        "                generated_text = response_json['candidates'][0]['content']['parts'][0]['text']\n",
        "                return generated_text\n",
        "\n",
        "        except aiohttp.ClientResponseError as err:\n",
        "            error_message = f\"Gemini API HTTP Error ({err.status}): {err.message}\"\n",
        "            raise RuntimeError(error_message) from err\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"An unexpected error occurred during async API call: {e}\")\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        \"\"\"Sync fallback - runs the async version in an event loop.\"\"\"\n",
        "        return asyncio.get_event_loop().run_until_complete(\n",
        "            self._acall(prompt, stop, None, **kwargs)\n",
        "        )\n",
        "\n",
        "    async def _agenerate(\n",
        "        self,\n",
        "        prompts: List[str],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> LLMResult:\n",
        "        \"\"\"Async generation for multiple prompts.\"\"\"\n",
        "        generations = []\n",
        "        for prompt in prompts:\n",
        "            text = await self._acall(prompt, stop, run_manager, **kwargs)\n",
        "            generations.append([{\"text\": text}])\n",
        "        return LLMResult(generations=generations)\n",
        "\n",
        "    def _generate(\n",
        "        self,\n",
        "        prompts: List[str],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> LLMResult:\n",
        "        \"\"\"Sync generate fallback.\"\"\"\n",
        "        generations = []\n",
        "        for prompt in prompts:\n",
        "            text = self._call(prompt, stop, run_manager, **kwargs)\n",
        "            generations.append([{\"text\": text}])\n",
        "        return LLMResult(generations=generations)\n",
        "\n",
        "    async def close(self):\n",
        "        \"\"\"Close the aiohttp session.\"\"\"\n",
        "        if self._session and not self._session.closed:\n",
        "            await self._session.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZL2tGOgNlMg"
      },
      "outputs": [],
      "source": [
        "# --- 2. LangGraph AGENT DEFINITIONS (Async) ---\n",
        "\n",
        "# 2.1. Define the State for the Graph\n",
        "class AgentState(TypedDict):\n",
        "    \"\"\"\n",
        "    The state of the graph, holding data passed between nodes.\n",
        "    \"\"\"\n",
        "    item_name: str\n",
        "    complexity_level: str\n",
        "    score: Optional[float]\n",
        "    review_data: Optional[str]\n",
        "\n",
        "\n",
        "# --- JSON Schema Definitions (Mandatory for Gemini JSON mode) ---\n",
        "\n",
        "# Schema for Agent 1: Score\n",
        "SCORE_SCHEMA = {\n",
        "    \"type\": \"OBJECT\",\n",
        "    \"properties\": {\n",
        "        \"score\": {\"type\": \"NUMBER\", \"description\": \"A random technical score between 0.0 and 1.0.\"},\n",
        "    },\n",
        "    \"required\": [\"score\"],\n",
        "    \"propertyOrdering\": [\"score\"]\n",
        "}\n",
        "\n",
        "# Schema for Agent 2: Detailed Review\n",
        "REVIEW_SCHEMA = {\n",
        "    \"type\": \"OBJECT\",\n",
        "    \"properties\": {\n",
        "        \"review_text\": {\"type\": \"STRING\", \"description\": \"A concise, technical review.\"},\n",
        "        \"category\": {\"type\": \"STRING\", \"description\": \"The category of the review (e.g., 'Positive', 'Neutral', 'Negative').\"}\n",
        "    },\n",
        "    \"required\": [\"review_text\", \"category\"],\n",
        "    \"propertyOrdering\": [\"review_text\", \"category\"]\n",
        "}\n",
        "\n",
        "\n",
        "# 2.2. Async Node for Agent 1: Score Generator\n",
        "async def score_generator_node(state: AgentState) -> dict:\n",
        "    \"\"\"\n",
        "    Async: Generates a technical score (0.0 to 1.0) for the item.\n",
        "    \"\"\"\n",
        "    print(\"--- [Agent 1] Executing: Score Generator (ASYNC) ---\")\n",
        "\n",
        "    # 1. Initialize async LLM with Score Schema\n",
        "    llm_score_generator = AsyncCustomHTTPGemini(model_name=\"gemini-2.5-flash\", response_schema=SCORE_SCHEMA)\n",
        "\n",
        "    # 2. Construct Prompt\n",
        "    prompt_1 = PromptTemplate.from_template(\n",
        "        \"You are a technical analyst. Your task is to assign a random score between 0.0 and 1.0 to the '{item_name}' based on its complexity '{complexity_level}'. Output the result strictly in JSON format according to the schema.\"\n",
        "    )\n",
        "    prompt_value = prompt_1.format(\n",
        "        item_name=state['item_name'],\n",
        "        complexity_level=state['complexity_level']\n",
        "    )\n",
        "\n",
        "    # 3. Invoke LLM asynchronously\n",
        "    raw_json_output = await llm_score_generator.ainvoke(prompt_value)\n",
        "    await llm_score_generator.close()\n",
        "\n",
        "    # 4. Parse JSON and update state\n",
        "    try:\n",
        "        score_data = json.loads(raw_json_output)\n",
        "        score = score_data.get('score', 0.0)\n",
        "        print(f\"--- [Agent 1] Generated Score: {score} ---\")\n",
        "        return {\"score\": score}\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON from Agent 1: {e}. Falling back to score 0.5.\")\n",
        "        return {\"score\": 0.5}\n",
        "\n",
        "\n",
        "# 2.3. Async Node for Agent 2: Review Generator\n",
        "async def review_generator_node(state: AgentState) -> dict:\n",
        "    \"\"\"\n",
        "    Async: Generates a detailed review based on the generated score and context.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- [Agent 2] Executing: Review Generator (ASYNC) ---\")\n",
        "\n",
        "    # 1. Initialize async LLM with Review Schema\n",
        "    llm_review_generator = AsyncCustomHTTPGemini(model_name=\"gemini-2.5-flash\", response_schema=REVIEW_SCHEMA)\n",
        "\n",
        "    # 2. Construct Prompt\n",
        "    prompt_2 = PromptTemplate.from_template(\n",
        "        \"Generate a technical review for the item '{item_name}' which has a complexity of '{complexity_level}' and received a technical score of {score}. Your review must reflect this score. Output the review strictly in JSON format according to the schema.\"\n",
        "    )\n",
        "\n",
        "    prompt_value = prompt_2.format(\n",
        "        item_name=state['item_name'],\n",
        "        complexity_level=state['complexity_level'],\n",
        "        score=state['score']\n",
        "    )\n",
        "\n",
        "    # 3. Invoke LLM asynchronously\n",
        "    raw_json_output = await llm_review_generator.ainvoke(prompt_value)\n",
        "    await llm_review_generator.close()\n",
        "\n",
        "    # 4. Update state with the final review data\n",
        "    print(\"--- [Agent 2] Generated Review Data ---\")\n",
        "    return {\"review_data\": raw_json_output}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufdaa8ejNpdP"
      },
      "outputs": [],
      "source": [
        "# --- 2.4. Async LangGraph Setup and Execution ---\n",
        "\n",
        "async def main():\n",
        "    \"\"\"Main async function to run the graph.\"\"\"\n",
        "    print(\"--- LangGraph Async Custom HTTP Gemini Cascading Agent Example ---\")\n",
        "    print(\"NOTE: Requires 'langgraph' and 'aiohttp' installation.\")\n",
        "\n",
        "    # Ensure API Key is available before starting the graph\n",
        "    if not os.getenv(\"GEMINI_API_KEY\"):\n",
        "        print(\"\\nERROR: GEMINI_API_KEY environment variable not set.\")\n",
        "        print(\"Please set the GEMINI_API_KEY environment variable and try again.\")\n",
        "        return\n",
        "\n",
        "    # --- Build the Graph ---\n",
        "    graph_builder = StateGraph(AgentState)\n",
        "\n",
        "    # Add the async nodes (agents) to the graph\n",
        "    graph_builder.add_node(\"score_generator\", score_generator_node)\n",
        "    graph_builder.add_node(\"review_generator\", review_generator_node)\n",
        "\n",
        "    # Set the entry point\n",
        "    graph_builder.set_entry_point(\"score_generator\")\n",
        "\n",
        "    # Define the sequence: Score Generator -> Review Generator -> END\n",
        "    graph_builder.add_edge(\"score_generator\", \"review_generator\")\n",
        "    graph_builder.add_edge(\"review_generator\", END)\n",
        "\n",
        "    # Compile the graph into a runnable application\n",
        "    app = graph_builder.compile()\n",
        "\n",
        "    # --- Example Invocation ---\n",
        "    print(\"\\nInvoking the two-agent LangGraph application (ASYNC)...\")\n",
        "\n",
        "    # This dictionary serves as the initial state for the graph\n",
        "    initial_state = {\n",
        "        \"item_name\": \"Quantum Entanglement Module v1.2\",\n",
        "        \"complexity_level\": \"High/Experimental\"\n",
        "    }\n",
        "\n",
        "    # Run the graph asynchronously\n",
        "    final_state = await app.ainvoke(initial_state)\n",
        "\n",
        "    final_review_json = final_state.get('review_data')\n",
        "    final_score = final_state.get('score')\n",
        "\n",
        "    print(\"\\n--- Graph Execution Complete ---\")\n",
        "    print(f\"Original Input:\\n{json.dumps(initial_state, indent=2)}\")\n",
        "    print(f\"Intermediate Score Generated by Agent 1: {final_score}\")\n",
        "\n",
        "    print(\"\\n--- Final Output (Generated by Agent 2) ---\")\n",
        "\n",
        "    # Try to print the final result nicely\n",
        "    try:\n",
        "        print(json.dumps(json.loads(final_review_json), indent=2))\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Raw Output:\\n{final_review_json}\")\n",
        "\n",
        "    print(\"\\n--- End of LangGraph Execution ---\")\n",
        "\n",
        "\n",
        "# Run the async main function (in Jupyter, use await directly)\n",
        "await main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMJPlGGjNvF-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
