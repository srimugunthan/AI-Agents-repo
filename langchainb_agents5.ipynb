{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUlJUiFKRO6x"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AKKmvhOmRQVy"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "from typing import Any, List, Optional\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.language_models.llms import BaseLLM\n",
        "from langchain_core.outputs import LLMResult\n",
        "\n",
        "from pydantic import BaseModel, Field, validator\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "from langchain_core.runnables import RunnableLambda # Added for mapping chain outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NOvjK1BWRGx3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- 1. Custom LLM Implementation ---\n",
        "\n",
        "class CustomHTTPGemini(BaseLLM):\n",
        "    \"\"\"\n",
        "    A custom LangChain LLM wrapper that interacts with the Google Gemini API\n",
        "    using direct HTTP requests (POST to generateContent endpoint).\n",
        "    \"\"\"\n",
        "\n",
        "    # Model and API Configuration\n",
        "    api_key: Optional[str] = None\n",
        "    model_name: str = Field(default=\"gemini-2.5-flash\", alias=\"model\")\n",
        "    # Base URL remains for configuration, though we construct the full endpoint in _call\n",
        "    base_url: str = \"https://generativelanguage.googleapis.com/v1beta/models/\"\n",
        "\n",
        "    def __init__(self, **kwargs: Any):\n",
        "        super().__init__(**kwargs)\n",
        "        # Ensure the API key is set, prioritizing the passed argument or environment variable\n",
        "        if not self.api_key:\n",
        "            self.api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "\n",
        "        if not self.api_key:\n",
        "            raise ValueError(\"GEMINI_API_KEY must be provided or set as an environment variable.\")\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"Return type of LLM.\"\"\"\n",
        "        return \"custom_http_gemini\"\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[Any] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        The core logic to make the HTTP POST request to the Gemini API.\n",
        "\n",
        "        This method is called by the LangChain framework when the LLM is invoked.\n",
        "        \"\"\"\n",
        "        # 1. Construct the API Endpoint for the specific model and method\n",
        "        # This now explicitly defines the full endpoint structure similar to the reference code.\n",
        "        api_endpoint = f\"{self.base_url}{self.model_name}:generateContent\"\n",
        "\n",
        "        # 2. Construct the complete URL with API Key as query parameter\n",
        "        url = f\"{api_endpoint}?key={self.api_key}\"\n",
        "\n",
        "        # 3. Define the HTTP headers\n",
        "        headers = {\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "\n",
        "        # 4. Construct the JSON request body following the Gemini API spec\n",
        "        request_data = {\n",
        "            \"contents\": [\n",
        "                {\n",
        "                    \"parts\": [\n",
        "                        {\n",
        "                            \"text\": prompt\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "            # Optional: Add generation config parameters if needed (e.g., temperature, max_output_tokens)\n",
        "            # \"generationConfig\": {\n",
        "            #     \"temperature\": 0.7\n",
        "            # }\n",
        "        }\n",
        "\n",
        "        # 5. Send the request\n",
        "        try:\n",
        "            # Using 'json=request_data' is a cleaner way to send JSON data with requests\n",
        "            response = requests.post(\n",
        "                url=url,\n",
        "                headers=headers,\n",
        "                json=request_data\n",
        "            )\n",
        "            response.raise_for_status() # Raise exception for bad status codes\n",
        "\n",
        "            response_json = response.json()\n",
        "\n",
        "            # 6. Extract the generated text from the structured JSON response\n",
        "            # Path: candidates[0].content.parts[0].text\n",
        "            generated_text = response_json['candidates'][0]['content']['parts'][0]['text']\n",
        "\n",
        "            return generated_text\n",
        "\n",
        "        except requests.exceptions.HTTPError as err:\n",
        "            error_message = f\"Gemini API HTTP Error ({err.response.status_code}): {err.response.text}\"\n",
        "            raise RuntimeError(error_message) from err\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"An unexpected error occurred during API call: {e}\")\n",
        "\n",
        "    # Note: _generate is required by BaseLLM if _call is not implemented, but since\n",
        "    # we implemented _call for simplicity, we provide a basic _generate for completeness\n",
        "    # in case of future changes in the base class.\n",
        "    def _generate(\n",
        "        self,\n",
        "        prompts: List[str],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[Any] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> LLMResult:\n",
        "        \"\"\"Call the LLM on a list of prompts.\"\"\"\n",
        "        generations = []\n",
        "        for prompt in prompts:\n",
        "            text = self._call(prompt, stop, run_manager, **kwargs)\n",
        "            generations.append([{\"text\": text}]) # Wrap the result in the expected structure\n",
        "        return LLMResult(generations=generations)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xv44VmflRkpk",
        "outputId": "ca15e0ea-a7b5-43fd-cf8b-a2b425df6b97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- LangChain Custom HTTP Gemini Cascading Agent Example ---\n",
            "\n",
            "Invoking the two-agent cascading chain: Summarizer -> Tweet Writer...\n",
            "\n",
            "Original Topic: The concept of cold fusion and why it is controversial\n",
            "\n",
            "--- Final Output (Generated by Chain 2 based on Chain 1's summary) ---\n",
            "Remember cold fusion? Imagine clean nuclear energy at room temp! ðŸ¤© Sounds amazing, right? But despite 1989 hype, inconsistent replication & lack of theory meant mainstream science largely cooled on it. A classic science controversy! #ColdFusionDebate #ScienceFacts #NuclearEnergy âš›ï¸\n",
            "\n",
            "--- End of Cascading Chain Execution ---\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- 2. LCEL Chain Integration (Cascading Agents) ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Setup ---\n",
        "    print(\"--- LangChain Custom HTTP Gemini Cascading Agent Example ---\")\n",
        "\n",
        "    # NOTE: Set your API Key in your environment before running:\n",
        "    # export GEMINI_API_KEY=\"YOUR_API_KEY_HERE\"\n",
        "\n",
        "    # Initialize the custom LLM\n",
        "    try:\n",
        "        # custom_llm will automatically pick up the API key from the environment variable\n",
        "        custom_llm = CustomHTTPGemini(model_name=\"gemini-2.5-flash\")\n",
        "    except ValueError as e:\n",
        "        print(f\"\\nERROR: {e}\")\n",
        "        print(\"Please set the GEMINI_API_KEY environment variable and try again.\")\n",
        "        exit()\n",
        "\n",
        "    # --- AGENT 1: Summarizer Chain (takes 'topic' as input) ---\n",
        "\n",
        "    prompt_1 = PromptTemplate.from_template(\n",
        "        \"Summarize the following concept into three key, concise bullet points, ensuring the output is easy to understand: {topic}\"\n",
        "    )\n",
        "\n",
        "    # Chain 1: Summarizer (outputs a string containing the summary)\n",
        "    chain_1 = prompt_1 | custom_llm | StrOutputParser()\n",
        "\n",
        "    # --- Mapping Step ---\n",
        "    # The output of Chain 1 is a string. Chain 2 needs a dictionary input {'summary': string}.\n",
        "    # This RunnableLambda maps the string output to the key expected by Chain 2.\n",
        "    def map_summary_to_context(summary_text: str) -> dict:\n",
        "        return {\"summary\": summary_text}\n",
        "\n",
        "    summary_mapper = RunnableLambda(map_summary_to_context)\n",
        "\n",
        "\n",
        "    # --- AGENT 2: Tweet Writer Chain (takes 'summary' as input) ---\n",
        "\n",
        "    prompt_2 = PromptTemplate.from_template(\n",
        "        \"Based on the summary below, write a creative and engaging tweet (max 280 characters) for a science blog. Include relevant hashtags and emojis.\\n\\nSUMMARY:\\n{summary}\"\n",
        "    )\n",
        "\n",
        "    # Chain 2: Tweet Writer (outputs a string containing the final tweet)\n",
        "    chain_2 = prompt_2 | custom_llm | StrOutputParser()\n",
        "\n",
        "    # --- FINAL CASCADING CHAIN ---\n",
        "    # Chain 1 -> Summary Mapper -> Chain 2\n",
        "    final_chain = chain_1 | summary_mapper | chain_2\n",
        "\n",
        "    # --- Example Invocation ---\n",
        "    print(\"\\nInvoking the two-agent cascading chain: Summarizer -> Tweet Writer...\")\n",
        "    topic_to_explain = \"The concept of cold fusion and why it is controversial\"\n",
        "\n",
        "    # The final chain only requires the input for the first chain's prompt ({topic})\n",
        "    response = final_chain.invoke({\"topic\": topic_to_explain})\n",
        "\n",
        "    print(f\"\\nOriginal Topic: {topic_to_explain}\")\n",
        "    print(\"\\n--- Final Output (Generated by Chain 2 based on Chain 1's summary) ---\")\n",
        "    print(response)\n",
        "    print(\"\\n--- End of Cascading Chain Execution ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeBVZzjpR3FB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
