{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uToE66ptNA81"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YASeMm1x-2Dn"
      },
      "outputs": [],
      "source": [
        "# Install aiohttp if not present\n",
        "!pip install aiohttp -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dH5H64_1NFJj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import json\n",
        "import time\n",
        "import statistics\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, List, Optional, TypedDict, Dict, Callable, Coroutine\n",
        "from contextlib import asynccontextmanager\n",
        "from functools import wraps\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.language_models.llms import BaseLLM\n",
        "from langchain_core.outputs import LLMResult\n",
        "from langchain_core.callbacks.manager import AsyncCallbackManagerForLLMRun, CallbackManagerForLLMRun\n",
        "from pydantic import Field\n",
        "from langgraph.graph import StateGraph, END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMNGaCJt-2Dq"
      },
      "source": [
        "## Async Instrumentation Module\n",
        "\n",
        "This module provides comprehensive latency and throughput measurement for async LangGraph agents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fhCC-wgq-2Dt"
      },
      "outputs": [],
      "source": [
        "# --- ASYNC INSTRUMENTATION CODE ---\n",
        "\n",
        "@dataclass\n",
        "class TimingRecord:\n",
        "    \"\"\"Single timing measurement record.\"\"\"\n",
        "    name: str\n",
        "    start_time: float\n",
        "    end_time: float\n",
        "    duration_ms: float\n",
        "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class LLMCallMetrics:\n",
        "    \"\"\"Metrics for a single LLM API call.\"\"\"\n",
        "    latency_ms: float\n",
        "    input_chars: int\n",
        "    output_chars: int\n",
        "    model_name: str\n",
        "    success: bool\n",
        "    is_async: bool = True\n",
        "    error_message: Optional[str] = None\n",
        "\n",
        "\n",
        "class AsyncInstrumentationCollector:\n",
        "    \"\"\"Collects and aggregates timing/throughput metrics for async operations.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.timing_records: List[TimingRecord] = []\n",
        "        self.llm_metrics: List[LLMCallMetrics] = []\n",
        "        self.node_timings: Dict[str, List[float]] = {}\n",
        "        self.graph_start_time: Optional[float] = None\n",
        "        self.graph_end_time: Optional[float] = None\n",
        "        self._run_count: int = 0\n",
        "        self._lock = asyncio.Lock() if asyncio.get_event_loop().is_running() else None\n",
        "\n",
        "    def _ensure_lock(self):\n",
        "        \"\"\"Ensure lock exists for thread safety in async context.\"\"\"\n",
        "        if self._lock is None:\n",
        "            try:\n",
        "                self._lock = asyncio.Lock()\n",
        "            except RuntimeError:\n",
        "                pass\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset all collected metrics for a new run.\"\"\"\n",
        "        self.timing_records = []\n",
        "        self.llm_metrics = []\n",
        "        self.node_timings = {}\n",
        "        self.graph_start_time = None\n",
        "        self.graph_end_time = None\n",
        "\n",
        "    def start_graph(self):\n",
        "        \"\"\"Mark the start of graph execution.\"\"\"\n",
        "        self.graph_start_time = time.perf_counter()\n",
        "        self._run_count += 1\n",
        "\n",
        "    def end_graph(self):\n",
        "        \"\"\"Mark the end of graph execution.\"\"\"\n",
        "        self.graph_end_time = time.perf_counter()\n",
        "\n",
        "    @asynccontextmanager\n",
        "    async def measure_async(self, name: str, **metadata):\n",
        "        \"\"\"Async context manager to measure execution time of an async code block.\"\"\"\n",
        "        start = time.perf_counter()\n",
        "        try:\n",
        "            yield\n",
        "        finally:\n",
        "            end = time.perf_counter()\n",
        "            duration_ms = (end - start) * 1000\n",
        "            record = TimingRecord(\n",
        "                name=name,\n",
        "                start_time=start,\n",
        "                end_time=end,\n",
        "                duration_ms=duration_ms,\n",
        "                metadata=metadata\n",
        "            )\n",
        "            self.timing_records.append(record)\n",
        "\n",
        "            if name not in self.node_timings:\n",
        "                self.node_timings[name] = []\n",
        "            self.node_timings[name].append(duration_ms)\n",
        "\n",
        "    def record_llm_call(self, metrics: LLMCallMetrics):\n",
        "        \"\"\"Record metrics from an LLM API call.\"\"\"\n",
        "        self.llm_metrics.append(metrics)\n",
        "\n",
        "    def get_total_latency_ms(self) -> float:\n",
        "        \"\"\"Get total graph execution latency in milliseconds.\"\"\"\n",
        "        if self.graph_start_time and self.graph_end_time:\n",
        "            return (self.graph_end_time - self.graph_start_time) * 1000\n",
        "        return 0.0\n",
        "\n",
        "    def get_llm_latency_stats(self) -> Dict[str, float]:\n",
        "        \"\"\"Get statistics on LLM call latencies.\"\"\"\n",
        "        if not self.llm_metrics:\n",
        "            return {}\n",
        "\n",
        "        latencies = [m.latency_ms for m in self.llm_metrics]\n",
        "        return {\n",
        "            \"count\": len(latencies),\n",
        "            \"total_ms\": sum(latencies),\n",
        "            \"mean_ms\": statistics.mean(latencies),\n",
        "            \"min_ms\": min(latencies),\n",
        "            \"max_ms\": max(latencies),\n",
        "            \"median_ms\": statistics.median(latencies),\n",
        "            \"stdev_ms\": statistics.stdev(latencies) if len(latencies) > 1 else 0.0,\n",
        "            \"async_calls\": sum(1 for m in self.llm_metrics if m.is_async)\n",
        "        }\n",
        "\n",
        "    def get_throughput_stats(self) -> Dict[str, float]:\n",
        "        \"\"\"Calculate throughput metrics.\"\"\"\n",
        "        total_time_sec = self.get_total_latency_ms() / 1000\n",
        "        if total_time_sec == 0:\n",
        "            return {}\n",
        "\n",
        "        total_input_chars = sum(m.input_chars for m in self.llm_metrics)\n",
        "        total_output_chars = sum(m.output_chars for m in self.llm_metrics)\n",
        "        num_requests = len(self.llm_metrics)\n",
        "\n",
        "        est_input_tokens = total_input_chars / 4\n",
        "        est_output_tokens = total_output_chars / 4\n",
        "\n",
        "        return {\n",
        "            \"requests_per_second\": num_requests / total_time_sec,\n",
        "            \"input_chars_per_second\": total_input_chars / total_time_sec,\n",
        "            \"output_chars_per_second\": total_output_chars / total_time_sec,\n",
        "            \"est_input_tokens_per_second\": est_input_tokens / total_time_sec,\n",
        "            \"est_output_tokens_per_second\": est_output_tokens / total_time_sec,\n",
        "            \"total_input_chars\": total_input_chars,\n",
        "            \"total_output_chars\": total_output_chars,\n",
        "            \"est_total_input_tokens\": int(est_input_tokens),\n",
        "            \"est_total_output_tokens\": int(est_output_tokens)\n",
        "        }\n",
        "\n",
        "    def get_node_breakdown(self) -> Dict[str, Dict[str, float]]:\n",
        "        \"\"\"Get timing breakdown by node.\"\"\"\n",
        "        breakdown = {}\n",
        "        for name, timings in self.node_timings.items():\n",
        "            breakdown[name] = {\n",
        "                \"count\": len(timings),\n",
        "                \"total_ms\": sum(timings),\n",
        "                \"mean_ms\": statistics.mean(timings),\n",
        "                \"min_ms\": min(timings),\n",
        "                \"max_ms\": max(timings)\n",
        "            }\n",
        "        return breakdown\n",
        "\n",
        "    def print_report(self):\n",
        "        \"\"\"Print a formatted instrumentation report.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ASYNC INSTRUMENTATION REPORT\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        print(f\"\\nðŸ“Š TOTAL LATENCY\")\n",
        "        print(f\"   Total Graph Execution: {self.get_total_latency_ms():.2f} ms\")\n",
        "\n",
        "        print(f\"\\nðŸ“ NODE-LEVEL BREAKDOWN\")\n",
        "        breakdown = self.get_node_breakdown()\n",
        "        for node_name, stats in breakdown.items():\n",
        "            print(f\"   {node_name}:\")\n",
        "            print(f\"      - Total: {stats['total_ms']:.2f} ms\")\n",
        "            print(f\"      - Mean:  {stats['mean_ms']:.2f} ms\")\n",
        "\n",
        "        print(f\"\\nðŸ¤– ASYNC LLM CALL STATISTICS\")\n",
        "        llm_stats = self.get_llm_latency_stats()\n",
        "        if llm_stats:\n",
        "            print(f\"   Number of calls:  {llm_stats['count']}\")\n",
        "            print(f\"   Async calls:      {llm_stats['async_calls']}\")\n",
        "            print(f\"   Total LLM time:   {llm_stats['total_ms']:.2f} ms\")\n",
        "            print(f\"   Mean latency:     {llm_stats['mean_ms']:.2f} ms\")\n",
        "            print(f\"   Min latency:      {llm_stats['min_ms']:.2f} ms\")\n",
        "            print(f\"   Max latency:      {llm_stats['max_ms']:.2f} ms\")\n",
        "            print(f\"   Median latency:   {llm_stats['median_ms']:.2f} ms\")\n",
        "\n",
        "        print(f\"\\nðŸš€ THROUGHPUT METRICS\")\n",
        "        throughput = self.get_throughput_stats()\n",
        "        if throughput:\n",
        "            print(f\"   Requests/second:       {throughput['requests_per_second']:.4f}\")\n",
        "            print(f\"   Output chars/second:   {throughput['output_chars_per_second']:.2f}\")\n",
        "            print(f\"   Est. output tokens/s:  {throughput['est_output_tokens_per_second']:.2f}\")\n",
        "            print(f\"   Total input chars:     {throughput['total_input_chars']}\")\n",
        "            print(f\"   Total output chars:    {throughput['total_output_chars']}\")\n",
        "\n",
        "        print(f\"\\nâ±ï¸  EXECUTION TIMELINE\")\n",
        "        for record in self.timing_records:\n",
        "            print(f\"   [{record.duration_ms:8.2f} ms] {record.name}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "        \"\"\"Export all metrics as a dictionary.\"\"\"\n",
        "        return {\n",
        "            \"total_latency_ms\": self.get_total_latency_ms(),\n",
        "            \"llm_statistics\": self.get_llm_latency_stats(),\n",
        "            \"throughput\": self.get_throughput_stats(),\n",
        "            \"node_breakdown\": self.get_node_breakdown(),\n",
        "            \"run_count\": self._run_count,\n",
        "            \"timeline\": [\n",
        "                {\"name\": r.name, \"duration_ms\": r.duration_ms, \"metadata\": r.metadata}\n",
        "                for r in self.timing_records\n",
        "            ]\n",
        "        }\n",
        "\n",
        "\n",
        "# Global collector instance\n",
        "collector = AsyncInstrumentationCollector()\n",
        "\n",
        "\n",
        "def instrumented_async_node(name: str):\n",
        "    \"\"\"Decorator to instrument an async LangGraph node function.\"\"\"\n",
        "    def decorator(func: Callable):\n",
        "        @wraps(func)\n",
        "        async def wrapper(state, *args, **kwargs):\n",
        "            async with collector.measure_async(name):\n",
        "                return await func(state, *args, **kwargs)\n",
        "        return wrapper\n",
        "    return decorator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbdwAvlALpCs",
        "outputId": "8b006871-aa6c-4ff1-9566-56307df30318"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1993174299.py:3: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
            "  class AsyncCustomHTTPGemini(BaseLLM):\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Async Custom LLM Implementation ---\n",
        "\n",
        "class AsyncCustomHTTPGemini(BaseLLM):\n",
        "    \"\"\"\n",
        "    An async custom LangChain LLM wrapper that interacts with the Google Gemini API\n",
        "    using aiohttp for asynchronous HTTP requests.\n",
        "    \"\"\"\n",
        "\n",
        "    api_key: Optional[str] = None\n",
        "    model_name: str = Field(default=\"gemini-2.5-flash\", alias=\"model\")\n",
        "    base_url: str = \"https://generativelanguage.googleapis.com/v1beta/models/\"\n",
        "    response_schema: Optional[dict] = None\n",
        "    _session: Optional[aiohttp.ClientSession] = None\n",
        "\n",
        "    class Config:\n",
        "        arbitrary_types_allowed = True\n",
        "\n",
        "    def __init__(self, **kwargs: Any):\n",
        "        super().__init__(**kwargs)\n",
        "        if not self.api_key:\n",
        "            self.api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "        if not self.api_key:\n",
        "            raise ValueError(\"GEMINI_API_KEY must be provided or set as an environment variable.\")\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"async_custom_http_gemini\"\n",
        "\n",
        "    async def _get_session(self) -> aiohttp.ClientSession:\n",
        "        \"\"\"Get or create an aiohttp session.\"\"\"\n",
        "        if self._session is None or self._session.closed:\n",
        "            self._session = aiohttp.ClientSession()\n",
        "        return self._session\n",
        "\n",
        "    async def _acall(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Async HTTP POST request to the Gemini API with instrumentation.\n",
        "        \"\"\"\n",
        "        api_endpoint = f\"{self.base_url}{self.model_name}:generateContent\"\n",
        "        url = f\"{api_endpoint}?key={self.api_key}\"\n",
        "        headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "        request_data = {\n",
        "            \"contents\": [{\"parts\": [{\"text\": prompt}]}]\n",
        "        }\n",
        "\n",
        "        if self.response_schema:\n",
        "            request_data[\"generationConfig\"] = {\n",
        "                \"responseMimeType\": \"application/json\",\n",
        "                \"responseSchema\": self.response_schema\n",
        "            }\n",
        "\n",
        "        input_chars = len(prompt)\n",
        "        start_time = time.perf_counter()\n",
        "\n",
        "        try:\n",
        "            session = await self._get_session()\n",
        "            async with session.post(url, headers=headers, json=request_data) as response:\n",
        "                response.raise_for_status()\n",
        "                response_json = await response.json()\n",
        "                generated_text = response_json['candidates'][0]['content']['parts'][0]['text']\n",
        "\n",
        "                end_time = time.perf_counter()\n",
        "                latency_ms = (end_time - start_time) * 1000\n",
        "\n",
        "                # Record async metrics\n",
        "                collector.record_llm_call(LLMCallMetrics(\n",
        "                    latency_ms=latency_ms,\n",
        "                    input_chars=input_chars,\n",
        "                    output_chars=len(generated_text),\n",
        "                    model_name=self.model_name,\n",
        "                    success=True,\n",
        "                    is_async=True\n",
        "                ))\n",
        "\n",
        "                return generated_text\n",
        "\n",
        "        except aiohttp.ClientResponseError as err:\n",
        "            end_time = time.perf_counter()\n",
        "            latency_ms = (end_time - start_time) * 1000\n",
        "\n",
        "            collector.record_llm_call(LLMCallMetrics(\n",
        "                latency_ms=latency_ms,\n",
        "                input_chars=input_chars,\n",
        "                output_chars=0,\n",
        "                model_name=self.model_name,\n",
        "                success=False,\n",
        "                is_async=True,\n",
        "                error_message=str(err)\n",
        "            ))\n",
        "\n",
        "            error_message = f\"Gemini API HTTP Error ({err.status}): {err.message}\"\n",
        "            raise RuntimeError(error_message) from err\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"An unexpected error occurred during async API call: {e}\")\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        \"\"\"Sync fallback - runs the async version in an event loop.\"\"\"\n",
        "        return asyncio.get_event_loop().run_until_complete(\n",
        "            self._acall(prompt, stop, None, **kwargs)\n",
        "        )\n",
        "\n",
        "    async def _agenerate(\n",
        "        self,\n",
        "        prompts: List[str],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> LLMResult:\n",
        "        \"\"\"Async generation for multiple prompts.\"\"\"\n",
        "        generations = []\n",
        "        for prompt in prompts:\n",
        "            text = await self._acall(prompt, stop, run_manager, **kwargs)\n",
        "            generations.append([{\"text\": text}])\n",
        "        return LLMResult(generations=generations)\n",
        "\n",
        "    def _generate(\n",
        "        self,\n",
        "        prompts: List[str],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> LLMResult:\n",
        "        \"\"\"Sync generate fallback.\"\"\"\n",
        "        generations = []\n",
        "        for prompt in prompts:\n",
        "            text = self._call(prompt, stop, run_manager, **kwargs)\n",
        "            generations.append([{\"text\": text}])\n",
        "        return LLMResult(generations=generations)\n",
        "\n",
        "    async def close(self):\n",
        "        \"\"\"Close the aiohttp session.\"\"\"\n",
        "        if self._session and not self._session.closed:\n",
        "            await self._session.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zZL2tGOgNlMg"
      },
      "outputs": [],
      "source": [
        "# --- 2. Async LangGraph AGENT DEFINITIONS ---\n",
        "\n",
        "# 2.1. Define the State for the Graph\n",
        "class AgentState(TypedDict):\n",
        "    item_name: str\n",
        "    complexity_level: str\n",
        "    score: Optional[float]\n",
        "    review_data: Optional[str]\n",
        "\n",
        "\n",
        "# --- JSON Schema Definitions ---\n",
        "SCORE_SCHEMA = {\n",
        "    \"type\": \"OBJECT\",\n",
        "    \"properties\": {\n",
        "        \"score\": {\"type\": \"NUMBER\", \"description\": \"A random technical score between 0.0 and 1.0.\"},\n",
        "    },\n",
        "    \"required\": [\"score\"],\n",
        "    \"propertyOrdering\": [\"score\"]\n",
        "}\n",
        "\n",
        "REVIEW_SCHEMA = {\n",
        "    \"type\": \"OBJECT\",\n",
        "    \"properties\": {\n",
        "        \"review_text\": {\"type\": \"STRING\", \"description\": \"A concise, technical review.\"},\n",
        "        \"category\": {\"type\": \"STRING\", \"description\": \"The category of the review (e.g., 'Positive', 'Neutral', 'Negative').\"}\n",
        "    },\n",
        "    \"required\": [\"review_text\", \"category\"],\n",
        "    \"propertyOrdering\": [\"review_text\", \"category\"]\n",
        "}\n",
        "\n",
        "\n",
        "# 2.2. Async Node for Agent 1: Score Generator\n",
        "@instrumented_async_node(\"score_generator_node\")\n",
        "async def score_generator_node(state: AgentState) -> dict:\n",
        "    \"\"\"Async: Generates a technical score (0.0 to 1.0) for the item.\"\"\"\n",
        "    print(\"--- [Agent 1] Executing: Score Generator (ASYNC) ---\")\n",
        "\n",
        "    llm_score_generator = AsyncCustomHTTPGemini(model_name=\"gemini-2.5-flash\", response_schema=SCORE_SCHEMA)\n",
        "\n",
        "    prompt_1 = PromptTemplate.from_template(\n",
        "        \"You are a technical analyst. Your task is to assign a random score between 0.0 and 1.0 to the '{item_name}' based on its complexity '{complexity_level}'. Output the result strictly in JSON format according to the schema.\"\n",
        "    )\n",
        "    prompt_value = prompt_1.format(\n",
        "        item_name=state['item_name'],\n",
        "        complexity_level=state['complexity_level']\n",
        "    )\n",
        "\n",
        "    # Use async invoke\n",
        "    raw_json_output = await llm_score_generator.ainvoke(prompt_value)\n",
        "    await llm_score_generator.close()  # Clean up session\n",
        "\n",
        "    try:\n",
        "        score_data = json.loads(raw_json_output)\n",
        "        score = score_data.get('score', 0.0)\n",
        "        print(f\"--- [Agent 1] Generated Score: {score} ---\")\n",
        "        return {\"score\": score}\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON from Agent 1: {e}. Falling back to score 0.5.\")\n",
        "        return {\"score\": 0.5}\n",
        "\n",
        "\n",
        "# 2.3. Async Node for Agent 2: Review Generator\n",
        "@instrumented_async_node(\"review_generator_node\")\n",
        "async def review_generator_node(state: AgentState) -> dict:\n",
        "    \"\"\"Async: Generates a detailed review based on the generated score and context.\"\"\"\n",
        "    print(\"\\n--- [Agent 2] Executing: Review Generator (ASYNC) ---\")\n",
        "\n",
        "    llm_review_generator = AsyncCustomHTTPGemini(model_name=\"gemini-2.5-flash\", response_schema=REVIEW_SCHEMA)\n",
        "\n",
        "    prompt_2 = PromptTemplate.from_template(\n",
        "        \"Generate a technical review for the item '{item_name}' which has a complexity of '{complexity_level}' and received a technical score of {score}. Your review must reflect this score. Output the review strictly in JSON format according to the schema.\"\n",
        "    )\n",
        "\n",
        "    prompt_value = prompt_2.format(\n",
        "        item_name=state['item_name'],\n",
        "        complexity_level=state['complexity_level'],\n",
        "        score=state['score']\n",
        "    )\n",
        "\n",
        "    # Use async invoke\n",
        "    raw_json_output = await llm_review_generator.ainvoke(prompt_value)\n",
        "    await llm_review_generator.close()  # Clean up session\n",
        "\n",
        "    print(\"--- [Agent 2] Generated Review Data ---\")\n",
        "    return {\"review_data\": raw_json_output}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufdaa8ejNpdP",
        "outputId": "8bf46789-700b-4232-f669-bf12ef909a3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- LangGraph Async Custom HTTP Gemini Cascading Agent Example ---\n",
            "NOTE: With ASYNC Instrumentation for Latency & Throughput Measurement\n",
            "\n",
            "Invoking the two-agent LangGraph application (ASYNC)...\n",
            "--- [Agent 1] Executing: Score Generator (ASYNC) ---\n",
            "--- [Agent 1] Generated Score: 0.732418 ---\n",
            "\n",
            "--- [Agent 2] Executing: Review Generator (ASYNC) ---\n",
            "--- [Agent 2] Generated Review Data ---\n",
            "\n",
            "--- Graph Execution Complete (ASYNC) ---\n",
            "Original Input:\n",
            "{\n",
            "  \"item_name\": \"Quantum Entanglement Module v1.2\",\n",
            "  \"complexity_level\": \"High/Experimental\"\n",
            "}\n",
            "Intermediate Score Generated by Agent 1: 0.732418\n",
            "\n",
            "--- Final Output (Generated by Agent 2) ---\n",
            "{\n",
            "  \"review_text\": \"The Quantum Entanglement Module v1.2 demonstrates promising entanglement fidelity, achieving a quantum state purity of 0.88 +/- 0.02 under controlled cryogenic conditions. Interfacing with existing quantum computing architectures remains experimental, exhibiting occasional decoherence issues during state transfer. While the high complexity warrants further optimization for robust, real-world integration, the core entanglement generation is commendable.\",\n",
            "  \"category\": \"Positive\"\n",
            "}\n",
            "\n",
            "============================================================\n",
            "ASYNC INSTRUMENTATION REPORT\n",
            "============================================================\n",
            "\n",
            "ðŸ“Š TOTAL LATENCY\n",
            "   Total Graph Execution: 3434.78 ms\n",
            "\n",
            "ðŸ“ NODE-LEVEL BREAKDOWN\n",
            "   score_generator_node:\n",
            "      - Total: 952.58 ms\n",
            "      - Mean:  952.58 ms\n",
            "   review_generator_node:\n",
            "      - Total: 2477.20 ms\n",
            "      - Mean:  2477.20 ms\n",
            "\n",
            "ðŸ¤– ASYNC LLM CALL STATISTICS\n",
            "   Number of calls:  2\n",
            "   Async calls:      2\n",
            "   Total LLM time:   3422.96 ms\n",
            "   Mean latency:     1711.48 ms\n",
            "   Min latency:      949.88 ms\n",
            "   Max latency:      2473.08 ms\n",
            "   Median latency:   1711.48 ms\n",
            "\n",
            "ðŸš€ THROUGHPUT METRICS\n",
            "   Requests/second:       0.5823\n",
            "   Output chars/second:   151.97\n",
            "   Est. output tokens/s:  37.99\n",
            "   Total input chars:     509\n",
            "   Total output chars:    522\n",
            "\n",
            "â±ï¸  EXECUTION TIMELINE\n",
            "   [  952.58 ms] score_generator_node\n",
            "   [ 2477.20 ms] review_generator_node\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# --- 2.4. Async LangGraph Setup and Execution ---\n",
        "\n",
        "async def run_async_graph():\n",
        "    \"\"\"Main async function to build and run the graph.\"\"\"\n",
        "    print(\"--- LangGraph Async Custom HTTP Gemini Cascading Agent Example ---\")\n",
        "    print(\"NOTE: With ASYNC Instrumentation for Latency & Throughput Measurement\")\n",
        "\n",
        "    if not os.getenv(\"GEMINI_API_KEY\"):\n",
        "        print(\"\\nERROR: GEMINI_API_KEY environment variable not set.\")\n",
        "        print(\"Please set the GEMINI_API_KEY environment variable and try again.\")\n",
        "        return None\n",
        "\n",
        "    # --- Build the Graph ---\n",
        "    graph_builder = StateGraph(AgentState)\n",
        "    graph_builder.add_node(\"score_generator\", score_generator_node)\n",
        "    graph_builder.add_node(\"review_generator\", review_generator_node)\n",
        "    graph_builder.set_entry_point(\"score_generator\")\n",
        "    graph_builder.add_edge(\"score_generator\", \"review_generator\")\n",
        "    graph_builder.add_edge(\"review_generator\", END)\n",
        "    app = graph_builder.compile()\n",
        "\n",
        "    # --- Example Invocation with Async Instrumentation ---\n",
        "    print(\"\\nInvoking the two-agent LangGraph application (ASYNC)...\")\n",
        "\n",
        "    initial_state = {\n",
        "        \"item_name\": \"Quantum Entanglement Module v1.2\",\n",
        "        \"complexity_level\": \"High/Experimental\"\n",
        "    }\n",
        "\n",
        "    # Reset collector and start timing\n",
        "    collector.reset()\n",
        "    collector.start_graph()\n",
        "\n",
        "    # Run the graph asynchronously\n",
        "    final_state = await app.ainvoke(initial_state)\n",
        "\n",
        "    # End timing\n",
        "    collector.end_graph()\n",
        "\n",
        "    final_review_json = final_state.get('review_data')\n",
        "    final_score = final_state.get('score')\n",
        "\n",
        "    print(\"\\n--- Graph Execution Complete (ASYNC) ---\")\n",
        "    print(f\"Original Input:\\n{json.dumps(initial_state, indent=2)}\")\n",
        "    print(f\"Intermediate Score Generated by Agent 1: {final_score}\")\n",
        "\n",
        "    print(\"\\n--- Final Output (Generated by Agent 2) ---\")\n",
        "    try:\n",
        "        print(json.dumps(json.loads(final_review_json), indent=2))\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Raw Output:\\n{final_review_json}\")\n",
        "\n",
        "    # --- Print Instrumentation Report ---\n",
        "    collector.print_report()\n",
        "\n",
        "    return app, final_state\n",
        "\n",
        "\n",
        "# Run the async graph\n",
        "# In Jupyter, we can use await directly\n",
        "result = await run_async_graph()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuIcOmjx-2D2"
      },
      "source": [
        "## Async Batch Throughput Testing\n",
        "\n",
        "Run multiple concurrent iterations to measure true async throughput benefits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "timc6gg--2D4"
      },
      "outputs": [],
      "source": [
        "async def run_single_invocation(app, test_case: Dict, run_id: int) -> Dict:\n",
        "    \"\"\"Run a single graph invocation and return metrics.\"\"\"\n",
        "    local_collector = AsyncInstrumentationCollector()\n",
        "\n",
        "    local_collector.reset()\n",
        "    local_collector.start_graph()\n",
        "\n",
        "    try:\n",
        "        _ = await app.ainvoke(test_case)\n",
        "        local_collector.end_graph()\n",
        "        return {\n",
        "            \"run_id\": run_id,\n",
        "            \"success\": True,\n",
        "            \"latency_ms\": local_collector.get_total_latency_ms(),\n",
        "            \"test_case\": test_case.get('item_name', 'Unknown')\n",
        "        }\n",
        "    except Exception as e:\n",
        "        local_collector.end_graph()\n",
        "        return {\n",
        "            \"run_id\": run_id,\n",
        "            \"success\": False,\n",
        "            \"latency_ms\": local_collector.get_total_latency_ms(),\n",
        "            \"error\": str(e),\n",
        "            \"test_case\": test_case.get('item_name', 'Unknown')\n",
        "        }\n",
        "\n",
        "\n",
        "async def run_async_batch_throughput_test(\n",
        "    test_cases: List[Dict],\n",
        "    num_iterations: int = 3,\n",
        "    concurrent: bool = True\n",
        "):\n",
        "    \"\"\"\n",
        "    Run async batch throughput test.\n",
        "\n",
        "    Args:\n",
        "        test_cases: List of initial states to test\n",
        "        num_iterations: Number of times to run each test case\n",
        "        concurrent: If True, run iterations concurrently within each test case\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"ASYNC BATCH THROUGHPUT TEST\")\n",
        "    print(f\"Running {len(test_cases)} test cases x {num_iterations} iterations\")\n",
        "    print(f\"Mode: {'CONCURRENT' if concurrent else 'SEQUENTIAL'}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Build the graph once\n",
        "    graph_builder = StateGraph(AgentState)\n",
        "    graph_builder.add_node(\"score_generator\", score_generator_node)\n",
        "    graph_builder.add_node(\"review_generator\", review_generator_node)\n",
        "    graph_builder.set_entry_point(\"score_generator\")\n",
        "    graph_builder.add_edge(\"score_generator\", \"review_generator\")\n",
        "    graph_builder.add_edge(\"review_generator\", END)\n",
        "    app = graph_builder.compile()\n",
        "\n",
        "    all_results = []\n",
        "    total_start = time.perf_counter()\n",
        "\n",
        "    run_id = 0\n",
        "    for i, test_case in enumerate(test_cases):\n",
        "        print(f\"\\nTest case {i+1}: {test_case.get('item_name', 'Unknown')}\")\n",
        "\n",
        "        if concurrent:\n",
        "            # Run iterations concurrently\n",
        "            tasks = [\n",
        "                run_single_invocation(app, test_case, run_id + j)\n",
        "                for j in range(num_iterations)\n",
        "            ]\n",
        "            results = await asyncio.gather(*tasks)\n",
        "            all_results.extend(results)\n",
        "\n",
        "            for r in results:\n",
        "                status = \"OK\" if r['success'] else f\"ERROR: {r.get('error', 'unknown')}\"\n",
        "                print(f\"  Run {r['run_id']}: {r['latency_ms']:.2f} ms - {status}\")\n",
        "        else:\n",
        "            # Run iterations sequentially\n",
        "            for j in range(num_iterations):\n",
        "                result = await run_single_invocation(app, test_case, run_id + j)\n",
        "                all_results.append(result)\n",
        "                status = \"OK\" if result['success'] else f\"ERROR: {result.get('error', 'unknown')}\"\n",
        "                print(f\"  Iteration {j+1}: {result['latency_ms']:.2f} ms - {status}\")\n",
        "\n",
        "        run_id += num_iterations\n",
        "\n",
        "    total_end = time.perf_counter()\n",
        "    total_time_sec = total_end - total_start\n",
        "\n",
        "    # Aggregate statistics\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"AGGREGATE RESULTS\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    successful_results = [r for r in all_results if r['success']]\n",
        "    latencies = [r['latency_ms'] for r in successful_results]\n",
        "\n",
        "    print(f\"\\nTotal runs attempted: {len(all_results)}\")\n",
        "    print(f\"Successful runs: {len(successful_results)}\")\n",
        "    print(f\"Total wall-clock time: {total_time_sec:.2f} seconds\")\n",
        "\n",
        "    if latencies:\n",
        "        print(f\"\\nEnd-to-end Latency (per run):\")\n",
        "        print(f\"  Mean:   {statistics.mean(latencies):.2f} ms\")\n",
        "        print(f\"  Median: {statistics.median(latencies):.2f} ms\")\n",
        "        print(f\"  Min:    {min(latencies):.2f} ms\")\n",
        "        print(f\"  Max:    {max(latencies):.2f} ms\")\n",
        "        if len(latencies) > 1:\n",
        "            print(f\"  Stdev:  {statistics.stdev(latencies):.2f} ms\")\n",
        "\n",
        "        print(f\"\\nThroughput:\")\n",
        "        print(f\"  Runs/second (wall-clock): {len(successful_results) / total_time_sec:.4f}\")\n",
        "\n",
        "        # Calculate theoretical sequential time\n",
        "        theoretical_sequential = sum(latencies) / 1000\n",
        "        speedup = theoretical_sequential / total_time_sec if total_time_sec > 0 else 1\n",
        "        print(f\"\\nAsync Performance:\")\n",
        "        print(f\"  Theoretical sequential time: {theoretical_sequential:.2f} sec\")\n",
        "        print(f\"  Actual wall-clock time:      {total_time_sec:.2f} sec\")\n",
        "        print(f\"  Speedup factor:              {speedup:.2f}x\")\n",
        "\n",
        "    return all_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWVHHjUs-2D6",
        "outputId": "52f0a1b7-a2fd-4ac3-ca3e-28d6555d3a04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "ASYNC BATCH THROUGHPUT TEST\n",
            "Running 3 test cases x 3 iterations\n",
            "Mode: CONCURRENT\n",
            "============================================================\n",
            "\n",
            "Test case 1: Quantum Entanglement Module v1.2\n",
            "--- [Agent 1] Executing: Score Generator (ASYNC) ---\n",
            "--- [Agent 1] Executing: Score Generator (ASYNC) ---\n",
            "--- [Agent 1] Executing: Score Generator (ASYNC) ---\n",
            "--- [Agent 1] Generated Score: 0.785 ---\n",
            "\n",
            "--- [Agent 2] Executing: Review Generator (ASYNC) ---\n",
            "--- [Agent 1] Generated Score: 0.75 ---\n",
            "\n",
            "--- [Agent 2] Executing: Review Generator (ASYNC) ---\n",
            "--- [Agent 1] Generated Score: 0.73 ---\n",
            "\n",
            "--- [Agent 2] Executing: Review Generator (ASYNC) ---\n",
            "--- [Agent 2] Generated Review Data ---\n",
            "  Run 0: 1347.43 ms - ERROR: Gemini API HTTP Error (429): Too Many Requests\n",
            "  Run 1: 1398.16 ms - ERROR: Gemini API HTTP Error (429): Too Many Requests\n",
            "  Run 2: 3862.58 ms - OK\n",
            "\n",
            "Test case 2: Basic Data Logger\n",
            "--- [Agent 1] Executing: Score Generator (ASYNC) ---\n",
            "--- [Agent 1] Executing: Score Generator (ASYNC) ---\n",
            "--- [Agent 1] Executing: Score Generator (ASYNC) ---\n",
            "  Run 3: 87.72 ms - ERROR: Gemini API HTTP Error (429): Too Many Requests\n",
            "  Run 4: 83.30 ms - ERROR: Gemini API HTTP Error (429): Too Many Requests\n",
            "  Run 5: 82.66 ms - ERROR: Gemini API HTTP Error (429): Too Many Requests\n",
            "\n",
            "Test case 3: ML Inference Pipeline\n",
            "--- [Agent 1] Executing: Score Generator (ASYNC) ---\n",
            "--- [Agent 1] Executing: Score Generator (ASYNC) ---\n",
            "--- [Agent 1] Executing: Score Generator (ASYNC) ---\n",
            "  Run 6: 81.37 ms - ERROR: Gemini API HTTP Error (429): Too Many Requests\n",
            "  Run 7: 87.78 ms - ERROR: Gemini API HTTP Error (429): Too Many Requests\n",
            "  Run 8: 89.53 ms - ERROR: Gemini API HTTP Error (429): Too Many Requests\n",
            "\n",
            "============================================================\n",
            "AGGREGATE RESULTS\n",
            "============================================================\n",
            "\n",
            "Total runs attempted: 9\n",
            "Successful runs: 1\n",
            "Total wall-clock time: 4.06 seconds\n",
            "\n",
            "End-to-end Latency (per run):\n",
            "  Mean:   3862.58 ms\n",
            "  Median: 3862.58 ms\n",
            "  Min:    3862.58 ms\n",
            "  Max:    3862.58 ms\n",
            "\n",
            "Throughput:\n",
            "  Runs/second (wall-clock): 0.2465\n",
            "\n",
            "Async Performance:\n",
            "  Theoretical sequential time: 3.86 sec\n",
            "  Actual wall-clock time:      4.06 sec\n",
            "  Speedup factor:              0.95x\n",
            "\n",
            "============================================================\n",
            "ASYNC BATCH THROUGHPUT TEST\n",
            "Running 3 test cases x 3 iterations\n",
            "Mode: SEQUENTIAL\n",
            "============================================================\n",
            "\n",
            "Test case 1: Quantum Entanglement Module v1.2\n",
            "--- [Agent 1] Executing: Score Generator (ASYNC) ---\n",
            "  Iteration 1: 74.47 ms - ERROR: Gemini API HTTP Error (429): Too Many Requests\n",
            "--- [Agent 1] Executing: Score Generator (ASYNC) ---\n",
            "  Iteration 2: 77.27 ms - ERROR: Gemini API HTTP Error (429): Too Many Requests\n",
            "--- [Agent 1] Executing: Score Generator (ASYNC) ---\n",
            "  Iteration 3: 82.39 ms - ERROR: Gemini API HTTP Error (429): Too Many Requests\n",
            "\n",
            "Test case 2: Basic Data Logger\n",
            "--- [Agent 1] Executing: Score Generator (ASYNC) ---\n",
            "  Iteration 1: 77.79 ms - ERROR: Gemini API HTTP Error (429): Too Many Requests\n",
            "--- [Agent 1] Executing: Score Generator (ASYNC) ---\n",
            "  Iteration 2: 84.09 ms - ERROR: Gemini API HTTP Error (429): Too Many Requests\n",
            "--- [Agent 1] Executing: Score Generator (ASYNC) ---\n",
            "  Iteration 3: 71.71 ms - ERROR: Gemini API HTTP Error (429): Too Many Requests\n",
            "\n",
            "Test case 3: ML Inference Pipeline\n",
            "--- [Agent 1] Executing: Score Generator (ASYNC) ---\n",
            "  Iteration 1: 84.90 ms - ERROR: Gemini API HTTP Error (429): Too Many Requests\n",
            "--- [Agent 1] Executing: Score Generator (ASYNC) ---\n",
            "  Iteration 2: 71.79 ms - ERROR: Gemini API HTTP Error (429): Too Many Requests\n",
            "--- [Agent 1] Executing: Score Generator (ASYNC) ---\n",
            "  Iteration 3: 241.93 ms - ERROR: Gemini API HTTP Error (429): Too Many Requests\n",
            "\n",
            "============================================================\n",
            "AGGREGATE RESULTS\n",
            "============================================================\n",
            "\n",
            "Total runs attempted: 9\n",
            "Successful runs: 0\n",
            "Total wall-clock time: 0.87 seconds\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "test_cases = [\n",
        "    {\"item_name\": \"Quantum Entanglement Module v1.2\", \"complexity_level\": \"High/Experimental\"},\n",
        "    {\"item_name\": \"Basic Data Logger\", \"complexity_level\": \"Low/Simple\"},\n",
        "    {\"item_name\": \"ML Inference Pipeline\", \"complexity_level\": \"Medium/Standard\"},\n",
        "]\n",
        "\n",
        "# Run concurrent test\n",
        "concurrent_results = await run_async_batch_throughput_test(test_cases, num_iterations=3, concurrent=True)\n",
        "\n",
        "# Run sequential test for comparison\n",
        "sequential_results = await run_async_batch_throughput_test(test_cases, num_iterations=3, concurrent=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtv6lRuE-2D6"
      },
      "source": [
        "## Parallel Node Execution Example\n",
        "\n",
        "Demonstrates running independent nodes in parallel for maximum async benefit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RZphZp6j-2D6"
      },
      "outputs": [],
      "source": [
        "# --- Extended State for Parallel Execution ---\n",
        "\n",
        "class ParallelAgentState(TypedDict):\n",
        "    \"\"\"State for parallel agent execution.\"\"\"\n",
        "    item_name: str\n",
        "    complexity_level: str\n",
        "    score_a: Optional[float]\n",
        "    score_b: Optional[float]\n",
        "    combined_result: Optional[str]\n",
        "\n",
        "\n",
        "@instrumented_async_node(\"score_generator_a\")\n",
        "async def score_generator_a(state: ParallelAgentState) -> dict:\n",
        "    \"\"\"First parallel score generator.\"\"\"\n",
        "    print(\"--- [Agent A] Executing: Score Generator A (ASYNC) ---\")\n",
        "    llm = AsyncCustomHTTPGemini(model_name=\"gemini-2.5-flash\", response_schema=SCORE_SCHEMA)\n",
        "\n",
        "    prompt = f\"Assign a technical innovation score (0-1) for '{state['item_name']}' with complexity '{state['complexity_level']}'. Output JSON.\"\n",
        "    result = await llm.ainvoke(prompt)\n",
        "    await llm.close()\n",
        "\n",
        "    try:\n",
        "        score = json.loads(result).get('score', 0.5)\n",
        "    except:\n",
        "        score = 0.5\n",
        "    print(f\"--- [Agent A] Score: {score} ---\")\n",
        "    return {\"score_a\": score}\n",
        "\n",
        "\n",
        "@instrumented_async_node(\"score_generator_b\")\n",
        "async def score_generator_b(state: ParallelAgentState) -> dict:\n",
        "    \"\"\"Second parallel score generator.\"\"\"\n",
        "    print(\"--- [Agent B] Executing: Score Generator B (ASYNC) ---\")\n",
        "    llm = AsyncCustomHTTPGemini(model_name=\"gemini-2.5-flash\", response_schema=SCORE_SCHEMA)\n",
        "\n",
        "    prompt = f\"Assign a market readiness score (0-1) for '{state['item_name']}' with complexity '{state['complexity_level']}'. Output JSON.\"\n",
        "    result = await llm.ainvoke(prompt)\n",
        "    await llm.close()\n",
        "\n",
        "    try:\n",
        "        score = json.loads(result).get('score', 0.5)\n",
        "    except:\n",
        "        score = 0.5\n",
        "    print(f\"--- [Agent B] Score: {score} ---\")\n",
        "    return {\"score_b\": score}\n",
        "\n",
        "\n",
        "@instrumented_async_node(\"combiner_node\")\n",
        "async def combiner_node(state: ParallelAgentState) -> dict:\n",
        "    \"\"\"Combines results from parallel nodes.\"\"\"\n",
        "    print(\"\\n--- [Combiner] Executing: Result Combiner ---\")\n",
        "    avg_score = (state['score_a'] + state['score_b']) / 2\n",
        "    result = f\"Combined Score: {avg_score:.2f} (Innovation: {state['score_a']}, Market: {state['score_b']})\"\n",
        "    print(f\"--- [Combiner] {result} ---\")\n",
        "    return {\"combined_result\": result}\n",
        "\n",
        "\n",
        "async def run_parallel_graph():\n",
        "    \"\"\"Run graph with parallel node execution.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PARALLEL NODE EXECUTION EXAMPLE\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Build parallel graph\n",
        "    graph_builder = StateGraph(ParallelAgentState)\n",
        "\n",
        "    # Add nodes\n",
        "    graph_builder.add_node(\"score_a\", score_generator_a)\n",
        "    graph_builder.add_node(\"score_b\", score_generator_b)\n",
        "    graph_builder.add_node(\"combiner\", combiner_node)\n",
        "\n",
        "    # Set entry point - both score generators run from start\n",
        "    graph_builder.set_entry_point(\"score_a\")\n",
        "\n",
        "    # Parallel execution: both A and B can run, then combine\n",
        "    # Note: LangGraph handles this via edges; for true parallelism,\n",
        "    # you'd need conditional edges or a fan-out pattern\n",
        "    graph_builder.add_edge(\"score_a\", \"score_b\")\n",
        "    graph_builder.add_edge(\"score_b\", \"combiner\")\n",
        "    graph_builder.add_edge(\"combiner\", END)\n",
        "\n",
        "    app = graph_builder.compile()\n",
        "\n",
        "    initial_state = {\n",
        "        \"item_name\": \"Neural Interface Chip\",\n",
        "        \"complexity_level\": \"Very High/Research\"\n",
        "    }\n",
        "\n",
        "    collector.reset()\n",
        "    collector.start_graph()\n",
        "\n",
        "    final_state = await app.ainvoke(initial_state)\n",
        "\n",
        "    collector.end_graph()\n",
        "\n",
        "    print(f\"\\nFinal Result: {final_state.get('combined_result')}\")\n",
        "    collector.print_report()\n",
        "\n",
        "    return final_state\n",
        "\n",
        "\n",
        "# Uncomment to run parallel example\n",
        "# parallel_result = await run_parallel_graph()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqZoAqdc-2D7"
      },
      "source": [
        "## Export Metrics to JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8bGl1f8l-2D7"
      },
      "outputs": [],
      "source": [
        "def export_metrics_to_json(filename: str = \"async_metrics.json\"):\n",
        "    \"\"\"Export collected metrics to a JSON file.\"\"\"\n",
        "    metrics = collector.to_dict()\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(metrics, f, indent=2)\n",
        "    print(f\"Metrics exported to {filename}\")\n",
        "\n",
        "# Uncomment to export\n",
        "# export_metrics_to_json(\"langgraph_async_metrics.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fHIreEloAY1Z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
