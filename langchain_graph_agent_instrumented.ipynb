{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uToE66ptNA81"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dH5H64_1NFJj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "import statistics\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, List, Optional, TypedDict, Dict, Callable\n",
        "from contextlib import contextmanager\n",
        "from functools import wraps\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.language_models.llms import BaseLLM\n",
        "from langchain_core.outputs import LLMResult\n",
        "from pydantic import Field\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langgraph.graph import StateGraph, END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMbMYn0AoZF4"
      },
      "source": [
        "## Instrumentation Module\n",
        "\n",
        "This module provides comprehensive latency and throughput measurement for the LangGraph agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Tg_vItjPoZF5"
      },
      "outputs": [],
      "source": [
        "# --- INSTRUMENTATION CODE ---\n",
        "\n",
        "@dataclass\n",
        "class TimingRecord:\n",
        "    \"\"\"Single timing measurement record.\"\"\"\n",
        "    name: str\n",
        "    start_time: float\n",
        "    end_time: float\n",
        "    duration_ms: float\n",
        "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class LLMCallMetrics:\n",
        "    \"\"\"Metrics for a single LLM API call.\"\"\"\n",
        "    latency_ms: float\n",
        "    input_chars: int\n",
        "    output_chars: int\n",
        "    model_name: str\n",
        "    success: bool\n",
        "    error_message: Optional[str] = None\n",
        "\n",
        "\n",
        "class InstrumentationCollector:\n",
        "    \"\"\"Collects and aggregates timing/throughput metrics across the pipeline.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.timing_records: List[TimingRecord] = []\n",
        "        self.llm_metrics: List[LLMCallMetrics] = []\n",
        "        self.node_timings: Dict[str, List[float]] = {}\n",
        "        self.graph_start_time: Optional[float] = None\n",
        "        self.graph_end_time: Optional[float] = None\n",
        "        self._run_count: int = 0\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset all collected metrics for a new run.\"\"\"\n",
        "        self.timing_records = []\n",
        "        self.llm_metrics = []\n",
        "        self.node_timings = {}\n",
        "        self.graph_start_time = None\n",
        "        self.graph_end_time = None\n",
        "\n",
        "    def start_graph(self):\n",
        "        \"\"\"Mark the start of graph execution.\"\"\"\n",
        "        self.graph_start_time = time.perf_counter()\n",
        "        self._run_count += 1\n",
        "\n",
        "    def end_graph(self):\n",
        "        \"\"\"Mark the end of graph execution.\"\"\"\n",
        "        self.graph_end_time = time.perf_counter()\n",
        "\n",
        "    @contextmanager\n",
        "    def measure(self, name: str, **metadata):\n",
        "        \"\"\"Context manager to measure execution time of a code block.\"\"\"\n",
        "        start = time.perf_counter()\n",
        "        try:\n",
        "            yield\n",
        "        finally:\n",
        "            end = time.perf_counter()\n",
        "            duration_ms = (end - start) * 1000\n",
        "            record = TimingRecord(\n",
        "                name=name,\n",
        "                start_time=start,\n",
        "                end_time=end,\n",
        "                duration_ms=duration_ms,\n",
        "                metadata=metadata\n",
        "            )\n",
        "            self.timing_records.append(record)\n",
        "\n",
        "            # Also track by node name for aggregation\n",
        "            if name not in self.node_timings:\n",
        "                self.node_timings[name] = []\n",
        "            self.node_timings[name].append(duration_ms)\n",
        "\n",
        "    def record_llm_call(self, metrics: LLMCallMetrics):\n",
        "        \"\"\"Record metrics from an LLM API call.\"\"\"\n",
        "        self.llm_metrics.append(metrics)\n",
        "\n",
        "    def get_total_latency_ms(self) -> float:\n",
        "        \"\"\"Get total graph execution latency in milliseconds.\"\"\"\n",
        "        if self.graph_start_time and self.graph_end_time:\n",
        "            return (self.graph_end_time - self.graph_start_time) * 1000\n",
        "        return 0.0\n",
        "\n",
        "    def get_llm_latency_stats(self) -> Dict[str, float]:\n",
        "        \"\"\"Get statistics on LLM call latencies.\"\"\"\n",
        "        if not self.llm_metrics:\n",
        "            return {}\n",
        "\n",
        "        latencies = [m.latency_ms for m in self.llm_metrics]\n",
        "        return {\n",
        "            \"count\": len(latencies),\n",
        "            \"total_ms\": sum(latencies),\n",
        "            \"mean_ms\": statistics.mean(latencies),\n",
        "            \"min_ms\": min(latencies),\n",
        "            \"max_ms\": max(latencies),\n",
        "            \"median_ms\": statistics.median(latencies),\n",
        "            \"stdev_ms\": statistics.stdev(latencies) if len(latencies) > 1 else 0.0\n",
        "        }\n",
        "\n",
        "    def get_throughput_stats(self) -> Dict[str, float]:\n",
        "        \"\"\"Calculate throughput metrics.\"\"\"\n",
        "        total_time_sec = self.get_total_latency_ms() / 1000\n",
        "        if total_time_sec == 0:\n",
        "            return {}\n",
        "\n",
        "        total_input_chars = sum(m.input_chars for m in self.llm_metrics)\n",
        "        total_output_chars = sum(m.output_chars for m in self.llm_metrics)\n",
        "        num_requests = len(self.llm_metrics)\n",
        "\n",
        "        # Estimate tokens (rough: ~4 chars per token)\n",
        "        est_input_tokens = total_input_chars / 4\n",
        "        est_output_tokens = total_output_chars / 4\n",
        "\n",
        "        return {\n",
        "            \"requests_per_second\": num_requests / total_time_sec,\n",
        "            \"input_chars_per_second\": total_input_chars / total_time_sec,\n",
        "            \"output_chars_per_second\": total_output_chars / total_time_sec,\n",
        "            \"est_input_tokens_per_second\": est_input_tokens / total_time_sec,\n",
        "            \"est_output_tokens_per_second\": est_output_tokens / total_time_sec,\n",
        "            \"total_input_chars\": total_input_chars,\n",
        "            \"total_output_chars\": total_output_chars,\n",
        "            \"est_total_input_tokens\": int(est_input_tokens),\n",
        "            \"est_total_output_tokens\": int(est_output_tokens)\n",
        "        }\n",
        "\n",
        "    def get_node_breakdown(self) -> Dict[str, Dict[str, float]]:\n",
        "        \"\"\"Get timing breakdown by node.\"\"\"\n",
        "        breakdown = {}\n",
        "        for name, timings in self.node_timings.items():\n",
        "            breakdown[name] = {\n",
        "                \"count\": len(timings),\n",
        "                \"total_ms\": sum(timings),\n",
        "                \"mean_ms\": statistics.mean(timings),\n",
        "                \"min_ms\": min(timings),\n",
        "                \"max_ms\": max(timings)\n",
        "            }\n",
        "        return breakdown\n",
        "\n",
        "    def print_report(self):\n",
        "        \"\"\"Print a formatted instrumentation report.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"INSTRUMENTATION REPORT\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Total Latency\n",
        "        print(f\"\\nðŸ“Š TOTAL LATENCY\")\n",
        "        print(f\"   Total Graph Execution: {self.get_total_latency_ms():.2f} ms\")\n",
        "\n",
        "        # Node Breakdown\n",
        "        print(f\"\\nðŸ“ NODE-LEVEL BREAKDOWN\")\n",
        "        breakdown = self.get_node_breakdown()\n",
        "        for node_name, stats in breakdown.items():\n",
        "            print(f\"   {node_name}:\")\n",
        "            print(f\"      - Total: {stats['total_ms']:.2f} ms\")\n",
        "            print(f\"      - Mean:  {stats['mean_ms']:.2f} ms\")\n",
        "\n",
        "        # LLM Call Statistics\n",
        "        print(f\"\\nðŸ¤– LLM CALL STATISTICS\")\n",
        "        llm_stats = self.get_llm_latency_stats()\n",
        "        if llm_stats:\n",
        "            print(f\"   Number of calls: {llm_stats['count']}\")\n",
        "            print(f\"   Total LLM time:  {llm_stats['total_ms']:.2f} ms\")\n",
        "            print(f\"   Mean latency:    {llm_stats['mean_ms']:.2f} ms\")\n",
        "            print(f\"   Min latency:     {llm_stats['min_ms']:.2f} ms\")\n",
        "            print(f\"   Max latency:     {llm_stats['max_ms']:.2f} ms\")\n",
        "            print(f\"   Median latency:  {llm_stats['median_ms']:.2f} ms\")\n",
        "\n",
        "        # Throughput\n",
        "        print(f\"\\nðŸš€ THROUGHPUT METRICS\")\n",
        "        throughput = self.get_throughput_stats()\n",
        "        if throughput:\n",
        "            print(f\"   Requests/second:       {throughput['requests_per_second']:.4f}\")\n",
        "            print(f\"   Output chars/second:   {throughput['output_chars_per_second']:.2f}\")\n",
        "            print(f\"   Est. output tokens/s:  {throughput['est_output_tokens_per_second']:.2f}\")\n",
        "            print(f\"   Total input chars:     {throughput['total_input_chars']}\")\n",
        "            print(f\"   Total output chars:    {throughput['total_output_chars']}\")\n",
        "\n",
        "        # Detailed Timeline\n",
        "        print(f\"\\nâ±ï¸  EXECUTION TIMELINE\")\n",
        "        for record in self.timing_records:\n",
        "            print(f\"   [{record.duration_ms:8.2f} ms] {record.name}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "        \"\"\"Export all metrics as a dictionary.\"\"\"\n",
        "        return {\n",
        "            \"total_latency_ms\": self.get_total_latency_ms(),\n",
        "            \"llm_statistics\": self.get_llm_latency_stats(),\n",
        "            \"throughput\": self.get_throughput_stats(),\n",
        "            \"node_breakdown\": self.get_node_breakdown(),\n",
        "            \"run_count\": self._run_count,\n",
        "            \"timeline\": [\n",
        "                {\"name\": r.name, \"duration_ms\": r.duration_ms, \"metadata\": r.metadata}\n",
        "                for r in self.timing_records\n",
        "            ]\n",
        "        }\n",
        "\n",
        "\n",
        "# Global collector instance\n",
        "collector = InstrumentationCollector()\n",
        "\n",
        "\n",
        "def instrumented_node(name: str):\n",
        "    \"\"\"Decorator to instrument a LangGraph node function.\"\"\"\n",
        "    def decorator(func: Callable):\n",
        "        @wraps(func)\n",
        "        def wrapper(state, *args, **kwargs):\n",
        "            with collector.measure(name):\n",
        "                return func(state, *args, **kwargs)\n",
        "        return wrapper\n",
        "    return decorator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IbdwAvlALpCs"
      },
      "outputs": [],
      "source": [
        "# --- 1. Custom LLM Implementation (with Instrumentation) ---\n",
        "\n",
        "class CustomHTTPGemini(BaseLLM):\n",
        "    \"\"\"\n",
        "    A custom LangChain LLM wrapper that interacts with the Google Gemini API\n",
        "    using direct HTTP requests, with built-in instrumentation.\n",
        "    \"\"\"\n",
        "\n",
        "    api_key: Optional[str] = None\n",
        "    model_name: str = Field(default=\"gemini-2.5-flash\", alias=\"model\")\n",
        "    base_url: str = \"https://generativelanguage.googleapis.com/v1beta/models/\"\n",
        "    response_schema: Optional[dict] = None\n",
        "\n",
        "    def __init__(self, **kwargs: Any):\n",
        "        super().__init__(**kwargs)\n",
        "        if not self.api_key:\n",
        "            self.api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "        if not self.api_key:\n",
        "            raise ValueError(\"GEMINI_API_KEY must be provided or set as an environment variable.\")\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"custom_http_gemini\"\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[Any] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Make the HTTP POST request to the Gemini API with instrumentation.\n",
        "        \"\"\"\n",
        "        api_endpoint = f\"{self.base_url}{self.model_name}:generateContent\"\n",
        "        url = f\"{api_endpoint}?key={self.api_key}\"\n",
        "        headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "        request_data = {\n",
        "            \"contents\": [{\"parts\": [{\"text\": prompt}]}]\n",
        "        }\n",
        "\n",
        "        if self.response_schema:\n",
        "            request_data[\"generationConfig\"] = {\n",
        "                \"responseMimeType\": \"application/json\",\n",
        "                \"responseSchema\": self.response_schema\n",
        "            }\n",
        "\n",
        "        # --- INSTRUMENTATION: Measure LLM call ---\n",
        "        input_chars = len(prompt)\n",
        "        start_time = time.perf_counter()\n",
        "\n",
        "        try:\n",
        "            response = requests.post(url=url, headers=headers, json=request_data)\n",
        "            response.raise_for_status()\n",
        "            response_json = response.json()\n",
        "            generated_text = response_json['candidates'][0]['content']['parts'][0]['text']\n",
        "\n",
        "            end_time = time.perf_counter()\n",
        "            latency_ms = (end_time - start_time) * 1000\n",
        "\n",
        "            # Record metrics\n",
        "            collector.record_llm_call(LLMCallMetrics(\n",
        "                latency_ms=latency_ms,\n",
        "                input_chars=input_chars,\n",
        "                output_chars=len(generated_text),\n",
        "                model_name=self.model_name,\n",
        "                success=True\n",
        "            ))\n",
        "\n",
        "            return generated_text\n",
        "\n",
        "        except requests.exceptions.HTTPError as err:\n",
        "            end_time = time.perf_counter()\n",
        "            latency_ms = (end_time - start_time) * 1000\n",
        "\n",
        "            # Record failed call\n",
        "            collector.record_llm_call(LLMCallMetrics(\n",
        "                latency_ms=latency_ms,\n",
        "                input_chars=input_chars,\n",
        "                output_chars=0,\n",
        "                model_name=self.model_name,\n",
        "                success=False,\n",
        "                error_message=str(err)\n",
        "            ))\n",
        "\n",
        "            error_message = f\"Gemini API HTTP Error ({err.response.status_code}): {err.response.text}\"\n",
        "            raise RuntimeError(error_message) from err\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"An unexpected error occurred during API call: {e}\")\n",
        "\n",
        "    def _generate(\n",
        "        self,\n",
        "        prompts: List[str],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[Any] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> LLMResult:\n",
        "        generations = []\n",
        "        for prompt in prompts:\n",
        "            text = self._call(prompt, stop, run_manager, **kwargs)\n",
        "            generations.append([{\"text\": text}])\n",
        "        return LLMResult(generations=generations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zZL2tGOgNlMg"
      },
      "outputs": [],
      "source": [
        "# --- 2. LangGraph AGENT DEFINITIONS (with Instrumentation) ---\n",
        "\n",
        "# 2.1. Define the State for the Graph\n",
        "class AgentState(TypedDict):\n",
        "    item_name: str\n",
        "    complexity_level: str\n",
        "    score: Optional[float]\n",
        "    review_data: Optional[str]\n",
        "\n",
        "\n",
        "# --- JSON Schema Definitions ---\n",
        "SCORE_SCHEMA = {\n",
        "    \"type\": \"OBJECT\",\n",
        "    \"properties\": {\n",
        "        \"score\": {\"type\": \"NUMBER\", \"description\": \"A random technical score between 0.0 and 1.0.\"},\n",
        "    },\n",
        "    \"required\": [\"score\"],\n",
        "    \"propertyOrdering\": [\"score\"]\n",
        "}\n",
        "\n",
        "REVIEW_SCHEMA = {\n",
        "    \"type\": \"OBJECT\",\n",
        "    \"properties\": {\n",
        "        \"review_text\": {\"type\": \"STRING\", \"description\": \"A concise, technical review.\"},\n",
        "        \"category\": {\"type\": \"STRING\", \"description\": \"The category of the review (e.g., 'Positive', 'Neutral', 'Negative').\"}\n",
        "    },\n",
        "    \"required\": [\"review_text\", \"category\"],\n",
        "    \"propertyOrdering\": [\"review_text\", \"category\"]\n",
        "}\n",
        "\n",
        "\n",
        "# 2.2. Node for Agent 1: Score Generator (INSTRUMENTED)\n",
        "@instrumented_node(\"score_generator_node\")\n",
        "def score_generator_node(state: AgentState) -> dict:\n",
        "    \"\"\"Generates a technical score (0.0 to 1.0) for the item.\"\"\"\n",
        "    print(\"--- [Agent 1] Executing: Score Generator ---\")\n",
        "\n",
        "    llm_score_generator = CustomHTTPGemini(model_name=\"gemini-2.5-flash\", response_schema=SCORE_SCHEMA)\n",
        "\n",
        "    prompt_1 = PromptTemplate.from_template(\n",
        "        \"You are a technical analyst. Your task is to assign a random score between 0.0 and 1.0 to the '{item_name}' based on its complexity '{complexity_level}'. Output the result strictly in JSON format according to the schema.\"\n",
        "    )\n",
        "    prompt_value = prompt_1.format(\n",
        "        item_name=state['item_name'],\n",
        "        complexity_level=state['complexity_level']\n",
        "    )\n",
        "\n",
        "    raw_json_output = llm_score_generator.invoke(prompt_value)\n",
        "\n",
        "    try:\n",
        "        score_data = json.loads(raw_json_output)\n",
        "        score = score_data.get('score', 0.0)\n",
        "        print(f\"--- [Agent 1] Generated Score: {score} ---\")\n",
        "        return {\"score\": score}\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON from Agent 1: {e}. Falling back to score 0.5.\")\n",
        "        return {\"score\": 0.5}\n",
        "\n",
        "\n",
        "# 2.3. Node for Agent 2: Review Generator (INSTRUMENTED)\n",
        "@instrumented_node(\"review_generator_node\")\n",
        "def review_generator_node(state: AgentState) -> dict:\n",
        "    \"\"\"Generates a detailed review based on the generated score and context.\"\"\"\n",
        "    print(\"\\n--- [Agent 2] Executing: Review Generator ---\")\n",
        "\n",
        "    llm_review_generator = CustomHTTPGemini(model_name=\"gemini-2.5-flash\", response_schema=REVIEW_SCHEMA)\n",
        "\n",
        "    prompt_2 = PromptTemplate.from_template(\n",
        "        \"Generate a technical review for the item '{item_name}' which has a complexity of '{complexity_level}' and received a technical score of {score}. Your review must reflect this score. Output the review strictly in JSON format according to the schema.\"\n",
        "    )\n",
        "\n",
        "    prompt_value = prompt_2.format(\n",
        "        item_name=state['item_name'],\n",
        "        complexity_level=state['complexity_level'],\n",
        "        score=state['score']\n",
        "    )\n",
        "\n",
        "    raw_json_output = llm_review_generator.invoke(prompt_value)\n",
        "\n",
        "    print(\"--- [Agent 2] Generated Review Data ---\")\n",
        "    return {\"review_data\": raw_json_output}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufdaa8ejNpdP",
        "outputId": "1b45a37c-4e51-47a2-dca2-4f0b5c6ba837"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- LangGraph Custom HTTP Gemini Cascading Agent Example ---\n",
            "NOTE: With Instrumentation for Latency & Throughput Measurement\n",
            "\n",
            "Invoking the two-agent LangGraph application...\n",
            "--- [Agent 1] Executing: Score Generator ---\n",
            "--- [Agent 1] Generated Score: 0.73 ---\n",
            "\n",
            "--- [Agent 2] Executing: Review Generator ---\n",
            "--- [Agent 2] Generated Review Data ---\n",
            "\n",
            "--- Graph Execution Complete ---\n",
            "Original Input:\n",
            "{\n",
            "  \"item_name\": \"Quantum Entanglement Module v1.2\",\n",
            "  \"complexity_level\": \"High/Experimental\"\n",
            "}\n",
            "Intermediate Score Generated by Agent 1: 0.73\n",
            "\n",
            "--- Final Output (Generated by Agent 2) ---\n",
            "{\n",
            "  \"review_text\": \"The Quantum Entanglement Module v1.2, despite its High/Experimental classification, demonstrates promising capabilities in establishing stable entangled states. Achieving an entanglement fidelity of 0.73, it offers consistent operation, though further optimization in decoherence mitigation techniques would enhance its performance. Its architecture allows for relatively low-latency entanglement generation, however, integration with diverse QPU interfaces presents calibration challenges. This module provides a robust foundation for continued development in a complex quantum domain.\",\n",
            "  \"category\": \"Positive\"\n",
            "}\n",
            "\n",
            "============================================================\n",
            "INSTRUMENTATION REPORT\n",
            "============================================================\n",
            "\n",
            "ðŸ“Š TOTAL LATENCY\n",
            "   Total Graph Execution: 3882.72 ms\n",
            "\n",
            "ðŸ“ NODE-LEVEL BREAKDOWN\n",
            "   score_generator_node:\n",
            "      - Total: 1084.10 ms\n",
            "      - Mean:  1084.10 ms\n",
            "   review_generator_node:\n",
            "      - Total: 2795.32 ms\n",
            "      - Mean:  2795.32 ms\n",
            "\n",
            "ðŸ¤– LLM CALL STATISTICS\n",
            "   Number of calls: 2\n",
            "   Total LLM time:  3876.44 ms\n",
            "   Mean latency:    1938.22 ms\n",
            "   Min latency:     1082.31 ms\n",
            "   Max latency:     2794.13 ms\n",
            "   Median latency:  1938.22 ms\n",
            "\n",
            "ðŸš€ THROUGHPUT METRICS\n",
            "   Requests/second:       0.5151\n",
            "   Output chars/second:   165.61\n",
            "   Est. output tokens/s:  41.40\n",
            "   Total input chars:     505\n",
            "   Total output chars:    643\n",
            "\n",
            "â±ï¸  EXECUTION TIMELINE\n",
            "   [ 1084.10 ms] score_generator_node\n",
            "   [ 2795.32 ms] review_generator_node\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# --- 2.4. LangGraph Setup and Execution (with Instrumentation) ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"--- LangGraph Custom HTTP Gemini Cascading Agent Example ---\")\n",
        "    print(\"NOTE: With Instrumentation for Latency & Throughput Measurement\")\n",
        "\n",
        "    if not os.getenv(\"GEMINI_API_KEY\"):\n",
        "        print(\"\\nERROR: GEMINI_API_KEY environment variable not set.\")\n",
        "        print(\"Please set the GEMINI_API_KEY environment variable and try again.\")\n",
        "        exit()\n",
        "\n",
        "    # --- Build the Graph ---\n",
        "    graph_builder = StateGraph(AgentState)\n",
        "    graph_builder.add_node(\"score_generator\", score_generator_node)\n",
        "    graph_builder.add_node(\"review_generator\", review_generator_node)\n",
        "    graph_builder.set_entry_point(\"score_generator\")\n",
        "    graph_builder.add_edge(\"score_generator\", \"review_generator\")\n",
        "    graph_builder.add_edge(\"review_generator\", END)\n",
        "    app = graph_builder.compile()\n",
        "\n",
        "    # --- Example Invocation with Instrumentation ---\n",
        "    print(\"\\nInvoking the two-agent LangGraph application...\")\n",
        "\n",
        "    initial_state = {\n",
        "        \"item_name\": \"Quantum Entanglement Module v1.2\",\n",
        "        \"complexity_level\": \"High/Experimental\"\n",
        "    }\n",
        "\n",
        "    # Reset collector and start timing\n",
        "    collector.reset()\n",
        "    collector.start_graph()\n",
        "\n",
        "    # Run the graph\n",
        "    final_state = app.invoke(initial_state)\n",
        "\n",
        "    # End timing\n",
        "    collector.end_graph()\n",
        "\n",
        "    final_review_json = final_state.get('review_data')\n",
        "    final_score = final_state.get('score')\n",
        "\n",
        "    print(\"\\n--- Graph Execution Complete ---\")\n",
        "    print(f\"Original Input:\\n{json.dumps(initial_state, indent=2)}\")\n",
        "    print(f\"Intermediate Score Generated by Agent 1: {final_score}\")\n",
        "\n",
        "    print(\"\\n--- Final Output (Generated by Agent 2) ---\")\n",
        "    try:\n",
        "        print(json.dumps(json.loads(final_review_json), indent=2))\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Raw Output:\\n{final_review_json}\")\n",
        "\n",
        "    # --- Print Instrumentation Report ---\n",
        "    collector.print_report()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX_EHNojoZF7"
      },
      "source": [
        "## Batch Throughput Testing\n",
        "\n",
        "Run multiple iterations to get more accurate throughput measurements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NTOaowSIoZF8"
      },
      "outputs": [],
      "source": [
        "def run_batch_throughput_test(app, test_cases: List[Dict], num_iterations: int = 5):\n",
        "    \"\"\"\n",
        "    Run multiple iterations to measure aggregate throughput.\n",
        "\n",
        "    Args:\n",
        "        app: Compiled LangGraph application\n",
        "        test_cases: List of initial states to test\n",
        "        num_iterations: Number of times to run each test case\n",
        "    \"\"\"\n",
        "    all_metrics = []\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"BATCH THROUGHPUT TEST\")\n",
        "    print(f\"Running {len(test_cases)} test cases x {num_iterations} iterations\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    total_start = time.perf_counter()\n",
        "\n",
        "    for i, test_case in enumerate(test_cases):\n",
        "        print(f\"\\nTest case {i+1}: {test_case.get('item_name', 'Unknown')}\")\n",
        "\n",
        "        for iteration in range(num_iterations):\n",
        "            collector.reset()\n",
        "            collector.start_graph()\n",
        "\n",
        "            try:\n",
        "                _ = app.invoke(test_case)\n",
        "            except Exception as e:\n",
        "                print(f\"  Iteration {iteration+1}: ERROR - {e}\")\n",
        "                continue\n",
        "\n",
        "            collector.end_graph()\n",
        "\n",
        "            metrics = collector.to_dict()\n",
        "            metrics['test_case_index'] = i\n",
        "            metrics['iteration'] = iteration\n",
        "            all_metrics.append(metrics)\n",
        "\n",
        "            print(f\"  Iteration {iteration+1}: {metrics['total_latency_ms']:.2f} ms\")\n",
        "\n",
        "    total_end = time.perf_counter()\n",
        "    total_time_sec = total_end - total_start\n",
        "\n",
        "    # Aggregate statistics\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"AGGREGATE RESULTS\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    latencies = [m['total_latency_ms'] for m in all_metrics]\n",
        "    llm_latencies = []\n",
        "    for m in all_metrics:\n",
        "        if m['llm_statistics']:\n",
        "            llm_latencies.append(m['llm_statistics'].get('mean_ms', 0))\n",
        "\n",
        "    successful_runs = len(all_metrics)\n",
        "    total_requests = sum(m['llm_statistics'].get('count', 0) for m in all_metrics if m['llm_statistics'])\n",
        "\n",
        "    print(f\"\\nTotal runs completed: {successful_runs}\")\n",
        "    print(f\"Total wall-clock time: {total_time_sec:.2f} seconds\")\n",
        "    print(f\"Total LLM requests: {total_requests}\")\n",
        "    print(f\"\\nEnd-to-end Latency (per run):\")\n",
        "    print(f\"  Mean:   {statistics.mean(latencies):.2f} ms\")\n",
        "    print(f\"  Median: {statistics.median(latencies):.2f} ms\")\n",
        "    print(f\"  Min:    {min(latencies):.2f} ms\")\n",
        "    print(f\"  Max:    {max(latencies):.2f} ms\")\n",
        "    if len(latencies) > 1:\n",
        "        print(f\"  Stdev:  {statistics.stdev(latencies):.2f} ms\")\n",
        "\n",
        "    if llm_latencies:\n",
        "        print(f\"\\nLLM Call Latency (mean per run):\")\n",
        "        print(f\"  Mean:   {statistics.mean(llm_latencies):.2f} ms\")\n",
        "        print(f\"  Median: {statistics.median(llm_latencies):.2f} ms\")\n",
        "\n",
        "    print(f\"\\nThroughput:\")\n",
        "    print(f\"  Runs/second:     {successful_runs / total_time_sec:.4f}\")\n",
        "    print(f\"  Requests/second: {total_requests / total_time_sec:.4f}\")\n",
        "\n",
        "    return all_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o38uNpc4oZF8",
        "outputId": "04f4dc5a-a40b-4cf0-88ca-34f3a9c2da9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "BATCH THROUGHPUT TEST\n",
            "Running 3 test cases x 3 iterations\n",
            "============================================================\n",
            "\n",
            "Test case 1: Quantum Entanglement Module v1.2\n",
            "--- [Agent 1] Executing: Score Generator ---\n",
            "--- [Agent 1] Generated Score: 0.76 ---\n",
            "\n",
            "--- [Agent 2] Executing: Review Generator ---\n",
            "--- [Agent 2] Generated Review Data ---\n",
            "  Iteration 1: 4708.23 ms\n",
            "--- [Agent 1] Executing: Score Generator ---\n",
            "--- [Agent 1] Generated Score: 0.7324 ---\n",
            "\n",
            "--- [Agent 2] Executing: Review Generator ---\n",
            "--- [Agent 2] Generated Review Data ---\n",
            "  Iteration 2: 4391.81 ms\n",
            "--- [Agent 1] Executing: Score Generator ---\n",
            "  Iteration 3: ERROR - Gemini API HTTP Error (429): {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-2.5-flash\\nPlease retry in 52.00482075s.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-2.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"5\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"52s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "Test case 2: Basic Data Logger\n",
            "--- [Agent 1] Executing: Score Generator ---\n",
            "  Iteration 1: ERROR - Gemini API HTTP Error (429): {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-2.5-flash\\nPlease retry in 51.773502013s.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-2.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"5\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"51s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "--- [Agent 1] Executing: Score Generator ---\n",
            "  Iteration 2: ERROR - Gemini API HTTP Error (429): {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-2.5-flash\\nPlease retry in 51.547406001s.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-2.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"5\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"51s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "--- [Agent 1] Executing: Score Generator ---\n",
            "  Iteration 3: ERROR - Gemini API HTTP Error (429): {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-2.5-flash\\nPlease retry in 51.325884499s.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-2.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"5\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"51s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "Test case 3: ML Inference Pipeline\n",
            "--- [Agent 1] Executing: Score Generator ---\n",
            "  Iteration 1: ERROR - Gemini API HTTP Error (429): {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-2.5-flash\\nPlease retry in 51.102422789s.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-2.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"5\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"51s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "--- [Agent 1] Executing: Score Generator ---\n",
            "  Iteration 2: ERROR - Gemini API HTTP Error (429): {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-2.5-flash\\nPlease retry in 50.870487745s.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-2.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"5\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"50s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "--- [Agent 1] Executing: Score Generator ---\n",
            "  Iteration 3: ERROR - Gemini API HTTP Error (429): {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-2.5-flash\\nPlease retry in 50.640471203s.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-2.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"5\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"50s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "============================================================\n",
            "AGGREGATE RESULTS\n",
            "============================================================\n",
            "\n",
            "Total runs completed: 2\n",
            "Total wall-clock time: 10.69 seconds\n",
            "Total LLM requests: 4\n",
            "\n",
            "End-to-end Latency (per run):\n",
            "  Mean:   4550.02 ms\n",
            "  Median: 4550.02 ms\n",
            "  Min:    4391.81 ms\n",
            "  Max:    4708.23 ms\n",
            "  Stdev:  223.74 ms\n",
            "\n",
            "LLM Call Latency (mean per run):\n",
            "  Mean:   2272.12 ms\n",
            "  Median: 2272.12 ms\n",
            "\n",
            "Throughput:\n",
            "  Runs/second:     0.1871\n",
            "  Requests/second: 0.3743\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "test_cases = [\n",
        "    {\"item_name\": \"Quantum Entanglement Module v1.2\", \"complexity_level\": \"High/Experimental\"},\n",
        "    {\"item_name\": \"Basic Data Logger\", \"complexity_level\": \"Low/Simple\"},\n",
        "    {\"item_name\": \"ML Inference Pipeline\", \"complexity_level\": \"Medium/Standard\"},\n",
        "]\n",
        "\n",
        "batch_results = run_batch_throughput_test(app, test_cases, num_iterations=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38y6kfYAoZF8"
      },
      "source": [
        "## Export Metrics to JSON\n",
        "\n",
        "Save metrics for external analysis or dashboards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "01qAb1HGoZF8"
      },
      "outputs": [],
      "source": [
        "# Export metrics to JSON file\n",
        "def export_metrics_to_json(filename: str = \"metrics.json\"):\n",
        "    \"\"\"Export collected metrics to a JSON file.\"\"\"\n",
        "    metrics = collector.to_dict()\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(metrics, f, indent=2)\n",
        "    print(f\"Metrics exported to {filename}\")\n",
        "\n",
        "# Uncomment to export\n",
        "# export_metrics_to_json(\"langgraph_metrics.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4e9NHaj95RnM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
