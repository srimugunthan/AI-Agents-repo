{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-N1kPWTYNyZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "40kX8P0uYhpU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "from typing import Any, List, Optional\n",
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.language_models.llms import BaseLLM\n",
        "from langchain_core.outputs import LLMResult\n",
        "\n",
        "from pydantic import BaseModel, Field, validator\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "from langchain_core.runnables import RunnableLambda # Added for mapping chain outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uaiCDZMGYQG7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- 1. Custom LLM Implementation ---\n",
        "\n",
        "class CustomHTTPGemini(BaseLLM):\n",
        "    \"\"\"\n",
        "    A custom LangChain LLM wrapper that interacts with the Google Gemini API\n",
        "    using direct HTTP requests (POST to generateContent endpoint), with optional\n",
        "    support for JSON output via response_schema.\n",
        "    \"\"\"\n",
        "\n",
        "    # Model and API Configuration\n",
        "    api_key: Optional[str] = None\n",
        "    model_name: str = Field(default=\"gemini-2.5-flash\", alias=\"model\")\n",
        "    base_url: str = \"https://generativelanguage.googleapis.com/v1beta/models/\"\n",
        "    # New field to hold the JSON schema definition for generationConfig\n",
        "    response_schema: Optional[dict] = None\n",
        "\n",
        "    def __init__(self, **kwargs: Any):\n",
        "        super().__init__(**kwargs)\n",
        "        # Ensure the API key is set, prioritizing the passed argument or environment variable\n",
        "        if not self.api_key:\n",
        "            self.api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "\n",
        "        if not self.api_key:\n",
        "            raise ValueError(\"GEMINI_API_KEY must be provided or set as an environment variable.\")\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"Return type of LLM.\"\"\"\n",
        "        return \"custom_http_gemini\"\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[Any] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        The core logic to make the HTTP POST request to the Gemini API.\n",
        "\n",
        "        This method is called by the LangChain framework when the LLM is invoked.\n",
        "        \"\"\"\n",
        "        # 1. Construct the API Endpoint for the specific model and method\n",
        "        api_endpoint = f\"{self.base_url}{self.model_name}:generateContent\"\n",
        "\n",
        "        # 2. Construct the complete URL with API Key as query parameter\n",
        "        url = f\"{api_endpoint}?key={self.api_key}\"\n",
        "\n",
        "        # 3. Define the HTTP headers\n",
        "        headers = {\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "\n",
        "        # 4. Construct the JSON request body following the Gemini API spec\n",
        "        request_data = {\n",
        "            \"contents\": [\n",
        "                {\n",
        "                    \"parts\": [\n",
        "                        {\n",
        "                            \"text\": prompt\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        # 5. Add generation configuration for JSON output if schema is present\n",
        "        if self.response_schema:\n",
        "            request_data[\"generationConfig\"] = {\n",
        "                \"responseMimeType\": \"application/json\",\n",
        "                \"responseSchema\": self.response_schema\n",
        "            }\n",
        "\n",
        "        # 6. Send the request\n",
        "        try:\n",
        "            # Using 'json=request_data' is a cleaner way to send JSON data with requests\n",
        "            response = requests.post(\n",
        "                url=url,\n",
        "                headers=headers,\n",
        "                json=request_data\n",
        "            )\n",
        "            response.raise_for_status() # Raise exception for bad status codes\n",
        "\n",
        "            response_json = response.json()\n",
        "\n",
        "            # 7. Extract the generated text from the structured JSON response\n",
        "            # Note: For JSON mode, the output text is the raw JSON string.\n",
        "            generated_text = response_json['candidates'][0]['content']['parts'][0]['text']\n",
        "\n",
        "            return generated_text\n",
        "\n",
        "        except requests.exceptions.HTTPError as err:\n",
        "            error_message = f\"Gemini API HTTP Error ({err.response.status_code}): {err.response.text}\"\n",
        "            raise RuntimeError(error_message) from err\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"An unexpected error occurred during API call: {e}\")\n",
        "\n",
        "    # Note: _generate is required by BaseLLM if _call is not implemented, but since\n",
        "    # we implemented _call for simplicity, we provide a basic _generate for completeness\n",
        "    # in case of future changes in the base class.\n",
        "    def _generate(\n",
        "        self,\n",
        "        prompts: List[str],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[Any] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> LLMResult:\n",
        "        \"\"\"Call the LLM on a list of prompts.\"\"\"\n",
        "        generations = []\n",
        "        for prompt in prompts:\n",
        "            text = self._call(prompt, stop, run_manager, **kwargs)\n",
        "            generations.append([{\"text\": text}]) # Wrap the result in the expected structure\n",
        "        return LLMResult(generations=generations)\n",
        "\n",
        "\n",
        "# --- 2. LCEL Chain Integration (Cascading Agents with JSON) ---\n",
        "\n",
        "# --- JSON Schema Definitions (Mandatory for Gemini JSON mode) ---\n",
        "\n",
        "# Schema for Agent 1: Score\n",
        "SCORE_SCHEMA = {\n",
        "    \"type\": \"OBJECT\",\n",
        "    \"properties\": {\n",
        "        \"score\": {\"type\": \"NUMBER\", \"description\": \"A random technical score between 0.0 and 1.0.\"},\n",
        "    },\n",
        "    \"required\": [\"score\"],\n",
        "    \"propertyOrdering\": [\"score\"]\n",
        "}\n",
        "\n",
        "# Schema for Agent 2: Detailed Review\n",
        "REVIEW_SCHEMA = {\n",
        "    \"type\": \"OBJECT\",\n",
        "    \"properties\": {\n",
        "        \"review_text\": {\"type\": \"STRING\", \"description\": \"A concise, technical review.\"},\n",
        "        \"category\": {\"type\": \"STRING\", \"description\": \"The category of the review (e.g., 'Positive', 'Neutral', 'Negative').\"}\n",
        "    },\n",
        "    \"required\": [\"review_text\", \"category\"],\n",
        "    \"propertyOrdering\": [\"review_text\", \"category\"]\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rdi1UGVYWLv",
        "outputId": "a25cfc86-f1e6-4ce0-c3b2-bba7fead95d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- LangChain Custom HTTP Gemini Cascading Agent with JSON Output Example ---\n",
            "\n",
            "Invoking the two-agent cascading chain: Score Generator -> Review Generator...\n",
            "\n",
            "Original Input:\n",
            "{\n",
            "  \"item_name\": \"Quantum Entanglement Module v1.2\",\n",
            "  \"complexity_level\": \"High/Experimental\"\n",
            "}\n",
            "\n",
            "--- Final Output (Generated by Agent 2 based on Agent 1's score) ---\n",
            "{\n",
            "  \"review_text\": \"The Quantum Entanglement Module v1.2 demonstrates promising entanglement fidelity (0.7825 E_F) within controlled experimental parameters. Its high complexity is inherent to the experimental nature, showing significant potential for quantum key distribution. However, further iteration is needed to enhance stability and reduce sensitivity to environmental noise for practical integration.\",\n",
            "  \"category\": \"Positive\"\n",
            "}\n",
            "\n",
            "--- End of Cascading Chain Execution ---\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Setup ---\n",
        "    print(\"--- LangChain Custom HTTP Gemini Cascading Agent with JSON Output Example ---\")\n",
        "\n",
        "    # NOTE: Set your API Key in your environment before running:\n",
        "    # export GEMINI_API_KEY=\"YOUR_API_KEY_HERE\"\n",
        "\n",
        "    # Initialize the custom LLM instances, each with its required JSON schema\n",
        "    try:\n",
        "        # LLM for the Score Generator (Agent 1)\n",
        "        llm_score_generator = CustomHTTPGemini(model_name=\"gemini-2.5-flash\", response_schema=SCORE_SCHEMA)\n",
        "        # LLM for the Review Generator (Agent 2)\n",
        "        llm_review_generator = CustomHTTPGemini(model_name=\"gemini-2.5-flash\", response_schema=REVIEW_SCHEMA)\n",
        "    except ValueError as e:\n",
        "        print(f\"\\nERROR: {e}\")\n",
        "        print(\"Please set the GEMINI_API_KEY environment variable and try again.\")\n",
        "        exit()\n",
        "\n",
        "    # --- AGENT 1: Score Generator (Input: JSON-like context, Output: JSON string with score) ---\n",
        "\n",
        "    prompt_1 = PromptTemplate.from_template(\n",
        "        \"You are a technical analyst. Your task is to assign a random score between 0.0 and 1.0 to the '{item_name}' based on its complexity '{complexity_level}'. Output the result strictly in JSON format according to the schema.\"\n",
        "    )\n",
        "\n",
        "    # Chain 1: Score Generator (outputs a raw JSON string)\n",
        "    chain_1 = prompt_1 | llm_score_generator | StrOutputParser()\n",
        "\n",
        "    # --- Mapping Step ---\n",
        "    # 1. Take the original input (which has 'item_name' and 'complexity_level') AND\n",
        "    #    the JSON score string output from Chain 1.\n",
        "    # 2. Parse the JSON score string.\n",
        "    # 3. Combine both into a single dictionary required by Chain 2.\n",
        "\n",
        "    def map_score_to_review_context(input_dict: dict) -> dict:\n",
        "        \"\"\"\n",
        "        Input dictionary looks like:\n",
        "        {'item_name': '...', 'complexity_level': '...', 'output': '{\"score\": 0.95}'}\n",
        "        \"\"\"\n",
        "        score_json_str = input_dict.pop(\"output\") # Get the raw JSON string output from Chain 1\n",
        "\n",
        "        try:\n",
        "            score_data = json.loads(score_json_str)\n",
        "            score = score_data.get('score', 0.0)\n",
        "\n",
        "            # Combine the original input context with the new score for Chain 2\n",
        "            return {\n",
        "                \"item_name\": input_dict[\"item_name\"],\n",
        "                \"score\": score,\n",
        "                \"complexity_level\": input_dict[\"complexity_level\"],\n",
        "            }\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Error decoding JSON from Agent 1: {e}\")\n",
        "            # Fallback for Chain 2 if JSON fails\n",
        "            return {\n",
        "                \"item_name\": input_dict[\"item_name\"],\n",
        "                \"score\": 0.5, # Default fallback score\n",
        "                \"complexity_level\": input_dict[\"complexity_level\"],\n",
        "            }\n",
        "\n",
        "    # Use RunnableParallel to ensure the output of Chain 1 is passed alongside the original input\n",
        "    # The output key is named 'output' by default if no key is specified.\n",
        "    score_with_context_chain = {\n",
        "        \"item_name\": RunnableLambda(lambda x: x[\"item_name\"]),\n",
        "        \"complexity_level\": RunnableLambda(lambda x: x[\"complexity_level\"]),\n",
        "        \"output\": chain_1 # Output of Chain 1: raw JSON score string\n",
        "    }\n",
        "\n",
        "    score_mapper = score_with_context_chain | RunnableLambda(map_score_to_review_context)\n",
        "\n",
        "\n",
        "    # --- AGENT 2: Review Generator (Input: score, item, complexity, Output: JSON string review) ---\n",
        "\n",
        "    prompt_2 = PromptTemplate.from_template(\n",
        "        \"Generate a technical review for the item '{item_name}' which has a complexity of '{complexity_level}' and received a technical score of {score}. Your review must reflect this score. Output the review strictly in JSON format according to the schema.\"\n",
        "    )\n",
        "\n",
        "    # Chain 2: Review Generator (outputs a raw JSON string)\n",
        "    chain_2 = prompt_2 | llm_review_generator | StrOutputParser()\n",
        "\n",
        "    # --- FINAL CASCADING CHAIN ---\n",
        "    # 1. Prompt 1 takes input {\"item_name\": ..., \"complexity_level\": ...}\n",
        "    # 2. Mapper executes Chain 1 and combines the score with original inputs.\n",
        "    # 3. Chain 2 receives the mapped inputs and generates the final review.\n",
        "    final_chain = score_mapper | chain_2\n",
        "\n",
        "    # --- Example Invocation ---\n",
        "    print(\"\\nInvoking the two-agent cascading chain: Score Generator -> Review Generator...\")\n",
        "\n",
        "    # This dictionary serves as the input for the entire final chain\n",
        "    input_topic = {\n",
        "        \"item_name\": \"Quantum Entanglement Module v1.2\",\n",
        "        \"complexity_level\": \"High/Experimental\"\n",
        "    }\n",
        "\n",
        "    # The final chain only requires the input for the first chain's prompt's variables\n",
        "    response = final_chain.invoke(input_topic)\n",
        "\n",
        "    print(f\"\\nOriginal Input:\\n{json.dumps(input_topic, indent=2)}\")\n",
        "    print(\"\\n--- Final Output (Generated by Agent 2 based on Agent 1's score) ---\")\n",
        "\n",
        "    # Try to print the final result nicely\n",
        "    try:\n",
        "        print(json.dumps(json.loads(response), indent=2))\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Raw Output:\\n{response}\")\n",
        "\n",
        "    print(\"\\n--- End of Cascading Chain Execution ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0zuwep5Y5Zj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
