{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uToE66ptNA81"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dH5H64_1NFJj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "from typing import Any, List, Optional, TypedDict\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.language_models.llms import BaseLLM\n",
        "from langchain_core.outputs import LLMResult\n",
        "from pydantic import Field  # Changed from langchain_core.pydantic_v1\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "# LangGraph Imports\n",
        "from langgraph.graph import StateGraph, END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbdwAvlALpCs"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# --- 1. Custom LLM Implementation ---\n",
        "\n",
        "class CustomHTTPGemini(BaseLLM):\n",
        "    \"\"\"\n",
        "    A custom LangChain LLM wrapper that interacts with the Google Gemini API\n",
        "    using direct HTTP requests (POST to generateContent endpoint), with optional\n",
        "    support for JSON output via response_schema.\n",
        "    \"\"\"\n",
        "\n",
        "    # Model and API Configuration\n",
        "    api_key: Optional[str] = None\n",
        "    model_name: str = Field(default=\"gemini-2.5-flash\", alias=\"model\")\n",
        "    base_url: str = \"https://generativelanguage.googleapis.com/v1beta/models/\"\n",
        "    # New field to hold the JSON schema definition for generationConfig\n",
        "    response_schema: Optional[dict] = None\n",
        "\n",
        "    def __init__(self, **kwargs: Any):\n",
        "        super().__init__(**kwargs)\n",
        "        # Ensure the API key is set, prioritizing the passed argument or environment variable\n",
        "        if not self.api_key:\n",
        "            self.api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "\n",
        "        if not self.api_key:\n",
        "            raise ValueError(\"GEMINI_API_KEY must be provided or set as an environment variable.\")\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"Return type of LLM.\"\"\"\n",
        "        return \"custom_http_gemini\"\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[Any] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        The core logic to make the HTTP POST request to the Gemini API.\n",
        "\n",
        "        This method is called by the LangChain framework when the LLM is invoked.\n",
        "        \"\"\"\n",
        "        # 1. Construct the API Endpoint for the specific model and method\n",
        "        api_endpoint = f\"{self.base_url}{self.model_name}:generateContent\"\n",
        "\n",
        "        # 2. Construct the complete URL with API Key as query parameter\n",
        "        url = f\"{api_endpoint}?key={self.api_key}\"\n",
        "\n",
        "        # 3. Define the HTTP headers\n",
        "        headers = {\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "\n",
        "        # 4. Construct the JSON request body following the Gemini API spec\n",
        "        request_data = {\n",
        "            \"contents\": [\n",
        "                {\n",
        "                    \"parts\": [\n",
        "                        {\n",
        "                            \"text\": prompt\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        # 5. Add generation configuration for JSON output if schema is present\n",
        "        if self.response_schema:\n",
        "            request_data[\"generationConfig\"] = {\n",
        "                \"responseMimeType\": \"application/json\",\n",
        "                \"responseSchema\": self.response_schema\n",
        "            }\n",
        "\n",
        "        # 6. Send the request\n",
        "        try:\n",
        "            # Using 'json=request_data' is a cleaner way to send JSON data with requests\n",
        "            response = requests.post(\n",
        "                url=url,\n",
        "                headers=headers,\n",
        "                json=request_data\n",
        "            )\n",
        "            response.raise_for_status() # Raise exception for bad status codes\n",
        "\n",
        "            response_json = response.json()\n",
        "\n",
        "            # 7. Extract the generated text from the structured JSON response\n",
        "            # Note: For JSON mode, the output text is the raw JSON string.\n",
        "            generated_text = response_json['candidates'][0]['content']['parts'][0]['text']\n",
        "\n",
        "            return generated_text\n",
        "\n",
        "        except requests.exceptions.HTTPError as err:\n",
        "            error_message = f\"Gemini API HTTP Error ({err.response.status_code}): {err.response.text}\"\n",
        "            raise RuntimeError(error_message) from err\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"An unexpected error occurred during API call: {e}\")\n",
        "\n",
        "    # Note: _generate is required by BaseLLM if _call is not implemented, but since\n",
        "    # we implemented _call for simplicity, we provide a basic _generate for completeness\n",
        "    # in case of future changes in the base class.\n",
        "    def _generate(\n",
        "        self,\n",
        "        prompts: List[str],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[Any] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> LLMResult:\n",
        "        \"\"\"Call the LLM on a list of prompts.\"\"\"\n",
        "        generations = []\n",
        "        for prompt in prompts:\n",
        "            text = self._call(prompt, stop, run_manager, **kwargs)\n",
        "            generations.append([{\"text\": text}]) # Wrap the result in the expected structure\n",
        "        return LLMResult(generations=generations)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZL2tGOgNlMg"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# --- 2. LangGraph AGENT DEFINITIONS (Replaces LCEL Cascading) ---\n",
        "\n",
        "# 2.1. Define the State for the Graph\n",
        "# TypedDict helps ensure predictable data flow between nodes\n",
        "class AgentState(TypedDict):\n",
        "    \"\"\"\n",
        "    The state of the graph, holding data passed between nodes.\n",
        "    \"\"\"\n",
        "    item_name: str\n",
        "    complexity_level: str\n",
        "    score: Optional[float]\n",
        "    review_data: Optional[str]\n",
        "\n",
        "\n",
        "# --- JSON Schema Definitions (Mandatory for Gemini JSON mode) ---\n",
        "\n",
        "# Schema for Agent 1: Score\n",
        "SCORE_SCHEMA = {\n",
        "    \"type\": \"OBJECT\",\n",
        "    \"properties\": {\n",
        "        \"score\": {\"type\": \"NUMBER\", \"description\": \"A random technical score between 0.0 and 1.0.\"},\n",
        "    },\n",
        "    \"required\": [\"score\"],\n",
        "    \"propertyOrdering\": [\"score\"]\n",
        "}\n",
        "\n",
        "# Schema for Agent 2: Detailed Review\n",
        "REVIEW_SCHEMA = {\n",
        "    \"type\": \"OBJECT\",\n",
        "    \"properties\": {\n",
        "        \"review_text\": {\"type\": \"STRING\", \"description\": \"A concise, technical review.\"},\n",
        "        \"category\": {\"type\": \"STRING\", \"description\": \"The category of the review (e.g., 'Positive', 'Neutral', 'Negative').\"}\n",
        "    },\n",
        "    \"required\": [\"review_text\", \"category\"],\n",
        "    \"propertyOrdering\": [\"review_text\", \"category\"]\n",
        "}\n",
        "\n",
        "\n",
        "# 2.2. Node for Agent 1: Score Generator\n",
        "def score_generator_node(state: AgentState) -> dict:\n",
        "    \"\"\"\n",
        "    Generates a technical score (0.0 to 1.0) for the item.\n",
        "    \"\"\"\n",
        "    print(\"--- [Agent 1] Executing: Score Generator ---\")\n",
        "\n",
        "    # 1. Initialize LLM with Score Schema\n",
        "    llm_score_generator = CustomHTTPGemini(model_name=\"gemini-2.5-flash\", response_schema=SCORE_SCHEMA)\n",
        "\n",
        "    # 2. Construct Prompt\n",
        "    prompt_1 = PromptTemplate.from_template(\n",
        "        \"You are a technical analyst. Your task is to assign a random score between 0.0 and 1.0 to the '{item_name}' based on its complexity '{complexity_level}'. Output the result strictly in JSON format according to the schema.\"\n",
        "    )\n",
        "    prompt_value = prompt_1.format(\n",
        "        item_name=state['item_name'],\n",
        "        complexity_level=state['complexity_level']\n",
        "    )\n",
        "\n",
        "    # 3. Invoke LLM\n",
        "    raw_json_output = llm_score_generator.invoke(prompt_value)\n",
        "\n",
        "    # 4. Parse JSON and update state\n",
        "    try:\n",
        "        score_data = json.loads(raw_json_output)\n",
        "        score = score_data.get('score', 0.0)\n",
        "        print(f\"--- [Agent 1] Generated Score: {score} ---\")\n",
        "        return {\"score\": score} # Return the update to the state\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON from Agent 1: {e}. Falling back to score 0.5.\")\n",
        "        return {\"score\": 0.5}\n",
        "\n",
        "\n",
        "# 2.3. Node for Agent 2: Review Generator\n",
        "def review_generator_node(state: AgentState) -> dict:\n",
        "    \"\"\"\n",
        "    Generates a detailed review based on the generated score and context.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- [Agent 2] Executing: Review Generator ---\")\n",
        "\n",
        "    # 1. Initialize LLM with Review Schema\n",
        "    llm_review_generator = CustomHTTPGemini(model_name=\"gemini-2.5-flash\", response_schema=REVIEW_SCHEMA)\n",
        "\n",
        "    # 2. Construct Prompt (uses 'score' from the state, generated by Agent 1)\n",
        "    prompt_2 = PromptTemplate.from_template(\n",
        "        \"Generate a technical review for the item '{item_name}' which has a complexity of '{complexity_level}' and received a technical score of {score}. Your review must reflect this score. Output the review strictly in JSON format according to the schema.\"\n",
        "    )\n",
        "\n",
        "    prompt_value = prompt_2.format(\n",
        "        item_name=state['item_name'],\n",
        "        complexity_level=state['complexity_level'],\n",
        "        score=state['score'] # Crucially, read the score from the state\n",
        "    )\n",
        "\n",
        "    # 3. Invoke LLM\n",
        "    raw_json_output = llm_review_generator.invoke(prompt_value)\n",
        "\n",
        "    # 4. Update state with the final review data (raw JSON string)\n",
        "    print(\"--- [Agent 2] Generated Review Data ---\")\n",
        "    return {\"review_data\": raw_json_output}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 3. Build the Graph ---\n",
        "\n",
        "def build_graph():\n",
        "    \"\"\"Build and compile the LangGraph.\"\"\"\n",
        "    graph_builder = StateGraph(AgentState)\n",
        "    graph_builder.add_node(\"score_generator\", score_generator_node)\n",
        "    graph_builder.add_node(\"review_generator\", review_generator_node)\n",
        "    graph_builder.set_entry_point(\"score_generator\")\n",
        "    graph_builder.add_edge(\"score_generator\", \"review_generator\")\n",
        "    graph_builder.add_edge(\"review_generator\", END)\n",
        "    return graph_builder.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufdaa8ejNpdP"
      },
      "outputs": [],
      "source": [
        "# --- 4. Single Request with Latency Measurement ---\n",
        "\n",
        "print(\"--- Single Request Execution ---\\n\")\n",
        "\n",
        "app = build_graph()\n",
        "\n",
        "initial_state = {\n",
        "    \"item_name\": \"Quantum Entanglement Module v1.2\",\n",
        "    \"complexity_level\": \"High/Experimental\"\n",
        "}\n",
        "\n",
        "# Measure latency\n",
        "start_time = time.perf_counter()\n",
        "final_state = app.invoke(initial_state)\n",
        "end_time = time.perf_counter()\n",
        "\n",
        "latency = end_time - start_time\n",
        "\n",
        "print(f\"\\n--- Result ---\")\n",
        "print(f\"Score: {final_state.get('score')}\")\n",
        "try:\n",
        "    print(f\"Review: {json.dumps(json.loads(final_state.get('review_data')), indent=2)}\")\n",
        "except:\n",
        "    print(f\"Review: {final_state.get('review_data')}\")\n",
        "\n",
        "print(f\"\\n--- Latency ---\")\n",
        "print(f\"Latency: {latency:.3f} seconds ({latency*1000:.1f} ms)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 5. Throughput Measurement ---\n",
        "\n",
        "def measure_throughput(app, initial_state: dict, num_requests: int = 5) -> dict:\n",
        "    \"\"\"\n",
        "    Run multiple requests and measure throughput.\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with latency and throughput metrics\n",
        "    \"\"\"\n",
        "    latencies = []\n",
        "    \n",
        "    print(f\"Running {num_requests} requests...\\n\")\n",
        "    \n",
        "    total_start = time.perf_counter()\n",
        "    \n",
        "    for i in range(num_requests):\n",
        "        start = time.perf_counter()\n",
        "        app.invoke(initial_state)\n",
        "        end = time.perf_counter()\n",
        "        \n",
        "        latency = end - start\n",
        "        latencies.append(latency)\n",
        "        print(f\"  Request {i+1}: {latency:.3f}s\")\n",
        "    \n",
        "    total_end = time.perf_counter()\n",
        "    total_time = total_end - total_start\n",
        "    \n",
        "    # Calculate metrics\n",
        "    avg_latency = sum(latencies) / len(latencies)\n",
        "    min_latency = min(latencies)\n",
        "    max_latency = max(latencies)\n",
        "    throughput = num_requests / total_time  # requests per second\n",
        "    \n",
        "    return {\n",
        "        \"num_requests\": num_requests,\n",
        "        \"total_time_sec\": total_time,\n",
        "        \"avg_latency_sec\": avg_latency,\n",
        "        \"min_latency_sec\": min_latency,\n",
        "        \"max_latency_sec\": max_latency,\n",
        "        \"throughput_rps\": throughput  # requests per second\n",
        "    }\n",
        "\n",
        "\n",
        "# --- Run throughput test ---\n",
        "print(\"--- Throughput Measurement ---\\n\")\n",
        "\n",
        "app = build_graph()\n",
        "\n",
        "initial_state = {\n",
        "    \"item_name\": \"Quantum Entanglement Module v1.2\",\n",
        "    \"complexity_level\": \"High/Experimental\"\n",
        "}\n",
        "\n",
        "metrics = measure_throughput(app, initial_state, num_requests=5)\n",
        "\n",
        "print(f\"\\n\" + \"=\"*40)\n",
        "print(\"RESULTS\")\n",
        "print(\"=\"*40)\n",
        "print(f\"Total requests:    {metrics['num_requests']}\")\n",
        "print(f\"Total time:        {metrics['total_time_sec']:.3f} sec\")\n",
        "print(f\"\")\n",
        "print(f\"Avg latency:       {metrics['avg_latency_sec']:.3f} sec ({metrics['avg_latency_sec']*1000:.1f} ms)\")\n",
        "print(f\"Min latency:       {metrics['min_latency_sec']:.3f} sec\")\n",
        "print(f\"Max latency:       {metrics['max_latency_sec']:.3f} sec\")\n",
        "print(f\"\")\n",
        "print(f\"Throughput:        {metrics['throughput_rps']:.4f} requests/second\")\n",
        "print(\"=\"*40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMJPlGGjNvF-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
