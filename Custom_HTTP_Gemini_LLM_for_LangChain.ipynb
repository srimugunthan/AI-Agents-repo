{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2PhbhN37_-3p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "IWkM9otoAK0c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "from typing import Any, List, Optional\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.language_models.llms import BaseLLM\n",
        "from langchain_core.outputs import LLMResult\n",
        "from pydantic import BaseModel, Field, validator\n",
        "from langchain_core.output_parsers import StrOutputParser\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "DFx0U1F8_9-N"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- 1. Custom LLM Implementation ---\n",
        "class CustomHTTPGemini(BaseLLM):\n",
        "    \"\"\"\n",
        "    A custom LangChain LLM wrapper that interacts with the Google Gemini API\n",
        "    using direct HTTP requests (POST to generateContent endpoint).\n",
        "    \"\"\"\n",
        "\n",
        "    # Model and API Configuration\n",
        "    api_key: Optional[str] = None\n",
        "    model_name: str = Field(default=\"gemini-2.5-flash\", alias=\"model\")\n",
        "    # Base URL remains for configuration, though we construct the full endpoint in _call\n",
        "    base_url: str = \"https://generativelanguage.googleapis.com/v1beta/models/\"\n",
        "\n",
        "    def __init__(self, **kwargs: Any):\n",
        "        super().__init__(**kwargs)\n",
        "        # Ensure the API key is set, prioritizing the passed argument or environment variable\n",
        "        if not self.api_key:\n",
        "            self.api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "\n",
        "        if not self.api_key:\n",
        "            raise ValueError(\"GEMINI_API_KEY must be provided or set as an environment variable.\")\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"Return type of LLM.\"\"\"\n",
        "        return \"custom_http_gemini\"\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[Any] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        The core logic to make the HTTP POST request to the Gemini API.\n",
        "        This method checks **kwargs for a pre-built 'request_data' payload.\n",
        "        \"\"\"\n",
        "\n",
        "        print(f\"\\n--- LLM Invoked for prompt (truncated): {prompt[:50]}... ---\")\n",
        "\n",
        "        # 1. Construct the API Endpoint for the specific model and method\n",
        "        api_endpoint = f\"{self.base_url}{self.model_name}:generateContent\"\n",
        "\n",
        "        # 2. Construct the complete URL with API Key as query parameter\n",
        "        url = f\"{api_endpoint}?key={self.api_key}\"\n",
        "\n",
        "        # 3. Define the HTTP headers\n",
        "        headers = {\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "\n",
        "        # --- NEW LOGIC: Determine Request Payload ---\n",
        "        request_data = kwargs.get(\"request_data\")\n",
        "        print(request_data)\n",
        "\n",
        "        # if request_data:\n",
        "        #     # Case A: Payload provided in kwargs (Constructed 'outside' _call)\n",
        "        #     # We must inject the dynamic prompt into the provided structure.\n",
        "        #     # We assume the user has provided a valid template with 'contents'.\n",
        "        #     if 'contents' in request_data and request_data['contents'] and 'parts' in request_data['contents'][0]:\n",
        "        #         print(\"Using custom request payload from **kwargs, injecting prompt.\")\n",
        "        #         # Insert the dynamic prompt into the designated text field\n",
        "        #         request_data['contents'][0]['parts'][0]['text'] = prompt\n",
        "        #     else:\n",
        "        #         raise ValueError(\"Custom request_data must contain the structure: ['contents'][0]['parts'][0]['text']\")\n",
        "        # else:\n",
        "        #     # Case B: Default payload construction (as in previous versions)\n",
        "        #     print(\"Using default request payload construction.\")\n",
        "        #     request_data = {\n",
        "        #         \"contents\": [\n",
        "        #             {\n",
        "        #                 \"parts\": [\n",
        "        #                     {\n",
        "        #                         \"text\": prompt\n",
        "        #                     }\n",
        "        #                 ]\n",
        "        #             }\n",
        "        #         ]\n",
        "        #     }\n",
        "        # --- END NEW LOGIC ---\n",
        "\n",
        "        # 4. Send the request\n",
        "        try:\n",
        "            # Using 'json=request_data' is a cleaner way to send JSON data with requests\n",
        "            response = requests.post(\n",
        "                url=url,\n",
        "                headers=headers,\n",
        "                json=request_data\n",
        "            )\n",
        "            response.raise_for_status() # Raise exception for bad status codes\n",
        "\n",
        "            response_json = response.json()\n",
        "\n",
        "            # 5. Extract the generated text from the structured JSON response\n",
        "            generated_text = response_json['candidates'][0]['content']['parts'][0]['text']\n",
        "\n",
        "            return generated_text\n",
        "\n",
        "        except requests.exceptions.HTTPError as err:\n",
        "            error_message = f\"Gemini API HTTP Error ({err.response.status_code}): {err.response.text}\"\n",
        "            raise RuntimeError(error_message) from err\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"An unexpected error occurred during API call: {e}\")\n",
        "\n",
        "    # Note: _generate is required by BaseLLM\n",
        "    def _generate(\n",
        "        self,\n",
        "        prompts: List[str],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[Any] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> LLMResult:\n",
        "        \"\"\"Call the LLM on a list of prompts.\"\"\"\n",
        "        generations = []\n",
        "        for prompt in prompts:\n",
        "            # Pass **kwargs through to _call\n",
        "            text = self._call(prompt, stop, run_manager, **kwargs)\n",
        "            generations.append([{\"text\": text}]) # Wrap the result in the expected structure\n",
        "        return LLMResult(generations=generations)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FI3B7_aOAd5z",
        "outputId": "51fedfdd-805d-4f87-cfeb-3ef078a7d75c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- LangChain Custom HTTP Gemini Example ---\n",
            "\n",
            "\n",
            "[DEMO 2] Running Direct Call (Passes payload via kwargs)\n",
            "\n",
            "--- LLM Invoked for prompt (truncated): You are a helpful assistant. Answer the following ... ---\n",
            "{'contents': [{'parts': [{'text': \"What are the first three lines of 'Jabberwocky'?\"}]}]}\n",
            "\n",
            "Question 2: What are the first three lines of 'Jabberwocky'?\n",
            "Model Response (Temp=0.3):\n",
            "The first three lines of 'Jabberwocky' are:\n",
            "\n",
            "'Twas brillig, and the slithy toves\n",
            "Did gyre and gimble in the wabe:\n",
            "All mimsy were the borogoves,\n",
            "\n",
            "--- End of Chain Execution ---\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# --- 2. LCEL Chain Integration and Demonstration ---\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Setup ---\n",
        "    print(\"--- LangChain Custom HTTP Gemini Example ---\")\n",
        "\n",
        "    # NOTE: Set your API Key in your environment before running:\n",
        "    # export GEMINI_API_KEY=\"YOUR_API_KEY_HERE\"\n",
        "\n",
        "    # Initialize the custom LLM\n",
        "    try:\n",
        "        custom_llm = CustomHTTPGemini(model_name=\"gemini-2.5-flash\")\n",
        "    except ValueError as e:\n",
        "        print(f\"\\nERROR: {e}\")\n",
        "        print(\"Please set the GEMINI_API_KEY environment variable and try again.\")\n",
        "        exit()\n",
        "\n",
        "    # Define a prompt template\n",
        "    prompt_template = PromptTemplate.from_template(\n",
        "        \"You are a helpful assistant. Answer the following question concisely: {question}\"\n",
        "    )\n",
        "\n",
        "    # -----------------------------------------------------------\n",
        "    # DEMONSTRATION 1: Standard LCEL Chain (Uses Default Payload Construction)\n",
        "    # -----------------------------------------------------------\n",
        "    # print(\"\\n[DEMO 1] Running Standard LCEL Chain (Uses default payload inside _call)\")\n",
        "\n",
        "    # chain = prompt_template | custom_llm | StrOutputParser()\n",
        "    # question_1 = \"What is the capital of France?\"\n",
        "\n",
        "    # try:\n",
        "    #     response_1 = chain.invoke({\"question\": question_1})\n",
        "    #     print(f\"\\nQuestion 1: {question_1}\")\n",
        "    #     print(f\"Model Response:\\n{response_1}\")\n",
        "    # except Exception as e:\n",
        "    #     print(f\"Demo 1 failed: {e}\")\n",
        "\n",
        "\n",
        "    # -----------------------------------------------------------\n",
        "    # DEMONSTRATION 2: Direct Call with Pre-constructed Payload\n",
        "    # This simulates construction *outside* the _call method.\n",
        "    # -----------------------------------------------------------\n",
        "    print(\"\\n\\n[DEMO 2] Running Direct Call (Passes payload via kwargs)\")\n",
        "\n",
        "    question_2 = \"What are the first three lines of 'Jabberwocky'?\"\n",
        "\n",
        "    # 1. Construct the payload structure outside the call (The argument you wanted passed)\n",
        "    # IMPORTANT: The prompt text is empty here; it will be injected by the _call method.\n",
        "    custom_request_payload = {\n",
        "        \"contents\": [\n",
        "            {\n",
        "                \"parts\": [\n",
        "                    {\n",
        "                        \"text\": question_2 # The LLM will overwrite this with the actual prompt\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # 2. Define the final prompt string (What LCEL would normally produce)\n",
        "    final_prompt_string = prompt_template.format(question=question_2)\n",
        "\n",
        "    try:\n",
        "        # 3. Call _call directly, passing the custom payload in **kwargs\n",
        "        # NOTE: This bypasses the complexity of LangChain's .invoke,\n",
        "        # but demonstrates the logic within _call().\n",
        "        response_2 = custom_llm._call(\n",
        "            prompt=final_prompt_string,\n",
        "            request_data=custom_request_payload # Passed in **kwargs\n",
        "        )\n",
        "        print(f\"\\nQuestion 2: {question_2}\")\n",
        "        print(f\"Model Response (Temp=0.3):\\n{response_2}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Demo 2 failed: {e}\")\n",
        "\n",
        "\n",
        "    print(\"\\n--- End of Chain Execution ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EaaaNG1AkKC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
