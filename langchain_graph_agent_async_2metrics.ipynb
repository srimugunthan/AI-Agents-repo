{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uToE66ptNA81"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install aiohttp -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dH5H64_1NFJj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import aiohttp\n",
        "import asyncio\n",
        "import json\n",
        "import time\n",
        "from typing import Any, List, Optional, TypedDict\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.language_models.llms import BaseLLM\n",
        "from langchain_core.outputs import LLMResult\n",
        "from langchain_core.callbacks.manager import AsyncCallbackManagerForLLMRun, CallbackManagerForLLMRun\n",
        "from pydantic import Field\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "# LangGraph Imports\n",
        "from langgraph.graph import StateGraph, END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbdwAvlALpCs"
      },
      "outputs": [],
      "source": [
        "# --- 1. Async Custom LLM Implementation ---\n",
        "\n",
        "class AsyncCustomHTTPGemini(BaseLLM):\n",
        "    \"\"\"\n",
        "    An async custom LangChain LLM wrapper that interacts with the Google Gemini API\n",
        "    using aiohttp for asynchronous HTTP requests.\n",
        "    \"\"\"\n",
        "\n",
        "    api_key: Optional[str] = None\n",
        "    model_name: str = Field(default=\"gemini-2.5-flash\", alias=\"model\")\n",
        "    base_url: str = \"https://generativelanguage.googleapis.com/v1beta/models/\"\n",
        "    response_schema: Optional[dict] = None\n",
        "    _session: Optional[aiohttp.ClientSession] = None\n",
        "\n",
        "    class Config:\n",
        "        arbitrary_types_allowed = True\n",
        "\n",
        "    def __init__(self, **kwargs: Any):\n",
        "        super().__init__(**kwargs)\n",
        "        if not self.api_key:\n",
        "            self.api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "        if not self.api_key:\n",
        "            raise ValueError(\"GEMINI_API_KEY must be provided or set as an environment variable.\")\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"async_custom_http_gemini\"\n",
        "\n",
        "    async def _get_session(self) -> aiohttp.ClientSession:\n",
        "        if self._session is None or self._session.closed:\n",
        "            self._session = aiohttp.ClientSession()\n",
        "        return self._session\n",
        "\n",
        "    async def _acall(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        api_endpoint = f\"{self.base_url}{self.model_name}:generateContent\"\n",
        "        url = f\"{api_endpoint}?key={self.api_key}\"\n",
        "        headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "        request_data = {\n",
        "            \"contents\": [{\"parts\": [{\"text\": prompt}]}]\n",
        "        }\n",
        "\n",
        "        if self.response_schema:\n",
        "            request_data[\"generationConfig\"] = {\n",
        "                \"responseMimeType\": \"application/json\",\n",
        "                \"responseSchema\": self.response_schema\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            session = await self._get_session()\n",
        "            async with session.post(url, headers=headers, json=request_data) as response:\n",
        "                response.raise_for_status()\n",
        "                response_json = await response.json()\n",
        "                generated_text = response_json['candidates'][0]['content']['parts'][0]['text']\n",
        "                return generated_text\n",
        "\n",
        "        except aiohttp.ClientResponseError as err:\n",
        "            error_message = f\"Gemini API HTTP Error ({err.status}): {err.message}\"\n",
        "            raise RuntimeError(error_message) from err\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"An unexpected error occurred during async API call: {e}\")\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        return asyncio.get_event_loop().run_until_complete(\n",
        "            self._acall(prompt, stop, None, **kwargs)\n",
        "        )\n",
        "\n",
        "    async def _agenerate(\n",
        "        self,\n",
        "        prompts: List[str],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> LLMResult:\n",
        "        generations = []\n",
        "        for prompt in prompts:\n",
        "            text = await self._acall(prompt, stop, run_manager, **kwargs)\n",
        "            generations.append([{\"text\": text}])\n",
        "        return LLMResult(generations=generations)\n",
        "\n",
        "    def _generate(\n",
        "        self,\n",
        "        prompts: List[str],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> LLMResult:\n",
        "        generations = []\n",
        "        for prompt in prompts:\n",
        "            text = self._call(prompt, stop, run_manager, **kwargs)\n",
        "            generations.append([{\"text\": text}])\n",
        "        return LLMResult(generations=generations)\n",
        "\n",
        "    async def close(self):\n",
        "        if self._session and not self._session.closed:\n",
        "            await self._session.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZL2tGOgNlMg"
      },
      "outputs": [],
      "source": [
        "# --- 2. LangGraph AGENT DEFINITIONS (Async) ---\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    item_name: str\n",
        "    complexity_level: str\n",
        "    score: Optional[float]\n",
        "    review_data: Optional[str]\n",
        "\n",
        "\n",
        "SCORE_SCHEMA = {\n",
        "    \"type\": \"OBJECT\",\n",
        "    \"properties\": {\n",
        "        \"score\": {\"type\": \"NUMBER\", \"description\": \"A random technical score between 0.0 and 1.0.\"},\n",
        "    },\n",
        "    \"required\": [\"score\"],\n",
        "    \"propertyOrdering\": [\"score\"]\n",
        "}\n",
        "\n",
        "REVIEW_SCHEMA = {\n",
        "    \"type\": \"OBJECT\",\n",
        "    \"properties\": {\n",
        "        \"review_text\": {\"type\": \"STRING\", \"description\": \"A concise, technical review.\"},\n",
        "        \"category\": {\"type\": \"STRING\", \"description\": \"The category of the review (e.g., 'Positive', 'Neutral', 'Negative').\"}\n",
        "    },\n",
        "    \"required\": [\"review_text\", \"category\"],\n",
        "    \"propertyOrdering\": [\"review_text\", \"category\"]\n",
        "}\n",
        "\n",
        "\n",
        "async def score_generator_node(state: AgentState) -> dict:\n",
        "    print(\"--- [Agent 1] Executing: Score Generator (ASYNC) ---\")\n",
        "    llm = AsyncCustomHTTPGemini(model_name=\"gemini-2.5-flash\", response_schema=SCORE_SCHEMA)\n",
        "\n",
        "    prompt = PromptTemplate.from_template(\n",
        "        \"You are a technical analyst. Your task is to assign a random score between 0.0 and 1.0 to the '{item_name}' based on its complexity '{complexity_level}'. Output the result strictly in JSON format according to the schema.\"\n",
        "    )\n",
        "    prompt_value = prompt.format(item_name=state['item_name'], complexity_level=state['complexity_level'])\n",
        "\n",
        "    raw_json_output = await llm.ainvoke(prompt_value)\n",
        "    await llm.close()\n",
        "\n",
        "    try:\n",
        "        score_data = json.loads(raw_json_output)\n",
        "        score = score_data.get('score', 0.0)\n",
        "        print(f\"--- [Agent 1] Generated Score: {score} ---\")\n",
        "        return {\"score\": score}\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON from Agent 1: {e}. Falling back to score 0.5.\")\n",
        "        return {\"score\": 0.5}\n",
        "\n",
        "\n",
        "async def review_generator_node(state: AgentState) -> dict:\n",
        "    print(\"\\n--- [Agent 2] Executing: Review Generator (ASYNC) ---\")\n",
        "    llm = AsyncCustomHTTPGemini(model_name=\"gemini-2.5-flash\", response_schema=REVIEW_SCHEMA)\n",
        "\n",
        "    prompt = PromptTemplate.from_template(\n",
        "        \"Generate a technical review for the item '{item_name}' which has a complexity of '{complexity_level}' and received a technical score of {score}. Your review must reflect this score. Output the review strictly in JSON format according to the schema.\"\n",
        "    )\n",
        "    prompt_value = prompt.format(\n",
        "        item_name=state['item_name'],\n",
        "        complexity_level=state['complexity_level'],\n",
        "        score=state['score']\n",
        "    )\n",
        "\n",
        "    raw_json_output = await llm.ainvoke(prompt_value)\n",
        "    await llm.close()\n",
        "\n",
        "    print(\"--- [Agent 2] Generated Review Data ---\")\n",
        "    return {\"review_data\": raw_json_output}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 3. Build the Graph ---\n",
        "\n",
        "def build_graph():\n",
        "    \"\"\"Build and compile the LangGraph.\"\"\"\n",
        "    graph_builder = StateGraph(AgentState)\n",
        "    graph_builder.add_node(\"score_generator\", score_generator_node)\n",
        "    graph_builder.add_node(\"review_generator\", review_generator_node)\n",
        "    graph_builder.set_entry_point(\"score_generator\")\n",
        "    graph_builder.add_edge(\"score_generator\", \"review_generator\")\n",
        "    graph_builder.add_edge(\"review_generator\", END)\n",
        "    return graph_builder.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufdaa8ejNpdP"
      },
      "outputs": [],
      "source": [
        "# --- 4. Single Request Execution with Latency Measurement ---\n",
        "\n",
        "async def run_single_request(app, initial_state: dict) -> tuple:\n",
        "    \"\"\"\n",
        "    Run a single query-request and measure latency.\n",
        "    \n",
        "    Returns:\n",
        "        (final_state, latency_seconds)\n",
        "    \"\"\"\n",
        "    start_time = time.perf_counter()\n",
        "    final_state = await app.ainvoke(initial_state)\n",
        "    end_time = time.perf_counter()\n",
        "    \n",
        "    latency = end_time - start_time\n",
        "    return final_state, latency\n",
        "\n",
        "\n",
        "# --- Run single request and show latency ---\n",
        "print(\"--- Single Request Execution ---\\n\")\n",
        "\n",
        "app = build_graph()\n",
        "\n",
        "initial_state = {\n",
        "    \"item_name\": \"Quantum Entanglement Module v1.2\",\n",
        "    \"complexity_level\": \"High/Experimental\"\n",
        "}\n",
        "\n",
        "final_state, latency = await run_single_request(app, initial_state)\n",
        "\n",
        "print(f\"\\n--- Result ---\")\n",
        "print(f\"Score: {final_state.get('score')}\")\n",
        "try:\n",
        "    print(f\"Review: {json.dumps(json.loads(final_state.get('review_data')), indent=2)}\")\n",
        "except:\n",
        "    print(f\"Review: {final_state.get('review_data')}\")\n",
        "\n",
        "print(f\"\\n--- Latency ---\")\n",
        "print(f\"Latency: {latency:.3f} seconds ({latency*1000:.1f} ms)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 5. Throughput Measurement ---\n",
        "\n",
        "async def measure_throughput(app, initial_state: dict, num_requests: int = 5) -> dict:\n",
        "    \"\"\"\n",
        "    Run multiple requests and measure throughput.\n",
        "    \n",
        "    Args:\n",
        "        app: Compiled LangGraph\n",
        "        initial_state: Input state for each request\n",
        "        num_requests: Number of requests to run\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with latency and throughput metrics\n",
        "    \"\"\"\n",
        "    latencies = []\n",
        "    \n",
        "    print(f\"Running {num_requests} requests...\\n\")\n",
        "    \n",
        "    total_start = time.perf_counter()\n",
        "    \n",
        "    for i in range(num_requests):\n",
        "        start = time.perf_counter()\n",
        "        await app.ainvoke(initial_state)\n",
        "        end = time.perf_counter()\n",
        "        \n",
        "        latency = end - start\n",
        "        latencies.append(latency)\n",
        "        print(f\"  Request {i+1}: {latency:.3f}s\")\n",
        "    \n",
        "    total_end = time.perf_counter()\n",
        "    total_time = total_end - total_start\n",
        "    \n",
        "    # Calculate metrics\n",
        "    avg_latency = sum(latencies) / len(latencies)\n",
        "    min_latency = min(latencies)\n",
        "    max_latency = max(latencies)\n",
        "    throughput = num_requests / total_time  # requests per second\n",
        "    \n",
        "    return {\n",
        "        \"num_requests\": num_requests,\n",
        "        \"total_time_sec\": total_time,\n",
        "        \"avg_latency_sec\": avg_latency,\n",
        "        \"min_latency_sec\": min_latency,\n",
        "        \"max_latency_sec\": max_latency,\n",
        "        \"throughput_rps\": throughput  # requests per second\n",
        "    }\n",
        "\n",
        "\n",
        "# --- Run throughput test ---\n",
        "print(\"--- Throughput Measurement ---\\n\")\n",
        "\n",
        "app = build_graph()\n",
        "\n",
        "initial_state = {\n",
        "    \"item_name\": \"Quantum Entanglement Module v1.2\",\n",
        "    \"complexity_level\": \"High/Experimental\"\n",
        "}\n",
        "\n",
        "metrics = await measure_throughput(app, initial_state, num_requests=5)\n",
        "\n",
        "print(f\"\\n\" + \"=\"*40)\n",
        "print(\"RESULTS\")\n",
        "print(\"=\"*40)\n",
        "print(f\"Total requests:    {metrics['num_requests']}\")\n",
        "print(f\"Total time:        {metrics['total_time_sec']:.3f} sec\")\n",
        "print(f\"\")\n",
        "print(f\"Avg latency:       {metrics['avg_latency_sec']:.3f} sec ({metrics['avg_latency_sec']*1000:.1f} ms)\")\n",
        "print(f\"Min latency:       {metrics['min_latency_sec']:.3f} sec\")\n",
        "print(f\"Max latency:       {metrics['max_latency_sec']:.3f} sec\")\n",
        "print(f\"\")\n",
        "print(f\"Throughput:        {metrics['throughput_rps']:.4f} requests/second\")\n",
        "print(\"=\"*40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMJPlGGjNvF-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
