{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bCu90hQnA5yj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uMDURmmEA5lx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "from typing import Any, List, Optional, TypedDict\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.language_models.llms import BaseLLM\n",
        "from langchain_core.outputs import LLMResult\n",
        "from pydantic import BaseModel, Field, validator\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "# LangGraph Imports\n",
        "from langgraph.graph import StateGraph, END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "c-E7xDNSA42S"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# --- 1. Custom LLM Implementation ---\n",
        "class CustomHTTPGemini(BaseLLM):\n",
        "    \"\"\"\n",
        "    A custom LangChain LLM wrapper that interacts with the Google Gemini API\n",
        "    using direct HTTP requests (POST to generateContent endpoint).\n",
        "    \"\"\"\n",
        "\n",
        "    # Model and API Configuration\n",
        "    api_key: Optional[str] = None\n",
        "    model_name: str = Field(default=\"gemini-2.5-flash\", alias=\"model\")\n",
        "    base_url: str = \"https://generativelanguage.googleapis.com/v1beta/models/\"\n",
        "\n",
        "    # NEW: Field to hold the JSON schema definition for generationConfig\n",
        "    response_schema: Optional[dict] = None\n",
        "\n",
        "    def __init__(self, **kwargs: Any):\n",
        "        super().__init__(**kwargs)\n",
        "        # Ensure the API key is set, prioritizing the passed argument or environment variable\n",
        "        if not self.api_key:\n",
        "            self.api_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "\n",
        "        if not self.api_key:\n",
        "            raise ValueError(\"GEMINI_API_KEY must be provided or set as an environment variable.\")\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"Return type of LLM.\"\"\"\n",
        "        return \"custom_http_gemini\"\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[Any] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        The core logic to make the HTTP POST request to the Gemini API.\n",
        "        This method now requires 'request_data' to be passed in kwargs.\n",
        "        \"\"\"\n",
        "\n",
        "        print(f\"\\n--- LLM Invoked for prompt (truncated): {prompt[:50]}... ---\")\n",
        "\n",
        "        # 1. Construct the API Endpoint for the specific model and method\n",
        "        api_endpoint = f\"{self.base_url}{self.model_name}:generateContent\"\n",
        "\n",
        "        # 2. Construct the complete URL with API Key as query parameter\n",
        "        url = f\"{api_endpoint}?key={self.api_key}\"\n",
        "\n",
        "        # 3. Define the HTTP headers\n",
        "        headers = {\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "\n",
        "        # --- Determine Request Payload (Now mandatory via kwargs) ---\n",
        "        request_data = kwargs.get(\"request_data\")\n",
        "\n",
        "        # Enforce mandatory presence and structure of request_data\n",
        "        if request_data is None:\n",
        "             raise ValueError(\"The 'request_data' dictionary must be explicitly passed in kwargs when calling _call().\")\n",
        "\n",
        "        # Case A: Payload provided in kwargs (Constructed 'outside' _call)\n",
        "        if 'contents' in request_data and request_data['contents'] and 'parts' in request_data['contents'][0]:\n",
        "            print(\"Using custom request payload from **kwargs, injecting prompt.\")\n",
        "            # Inject the prompt into the designated text field\n",
        "            request_data['contents'][0]['parts'][0]['text'] = prompt\n",
        "        else:\n",
        "            # If the structure is missing, raise an error since the assumption is\n",
        "            # the external caller provides a valid base structure.\n",
        "            raise ValueError(\"The passed 'request_data' must contain the structure: ['contents'][0]['parts'][0]['text'] where the prompt will be injected.\")\n",
        "\n",
        "        # --- END Request Payload Logic ---\n",
        "\n",
        "        # --- JSON GENERATION CONFIG LOGIC ---\n",
        "        # Prioritize schema from instance, then from kwargs\n",
        "        schema = self.response_schema or kwargs.get(\"response_schema\")\n",
        "\n",
        "        if schema:\n",
        "            print(f\"Applying JSON schema for structured output.\")\n",
        "            # Ensure generationConfig exists or create it\n",
        "            if \"generationConfig\" not in request_data:\n",
        "                request_data[\"generationConfig\"] = {}\n",
        "\n",
        "            request_data[\"generationConfig\"].update({\n",
        "                \"responseMimeType\": \"application/json\",\n",
        "                \"responseSchema\": schema\n",
        "            })\n",
        "        # --- END JSON CONFIG LOGIC ---\n",
        "\n",
        "\n",
        "        # 4. Send the request\n",
        "        try:\n",
        "            # Using 'json=request_data' is a cleaner way to send JSON data with requests\n",
        "            response = requests.post(\n",
        "                url=url,\n",
        "                headers=headers,\n",
        "                json=request_data\n",
        "            )\n",
        "            response.raise_for_status() # Raise exception for bad status codes\n",
        "\n",
        "            response_json = response.json()\n",
        "\n",
        "            # 5. Extract the generated text from the structured JSON response\n",
        "            generated_text = response_json['candidates'][0]['content']['parts'][0]['text']\n",
        "\n",
        "            return generated_text\n",
        "\n",
        "        except requests.exceptions.HTTPError as err:\n",
        "            error_message = f\"Gemini API HTTP Error ({err.response.status_code}): {err.response.text}\"\n",
        "            raise RuntimeError(error_message) from err\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"An unexpected error occurred during API call: {e}\")\n",
        "\n",
        "    # Note: _generate is required by BaseLLM\n",
        "    def _generate(\n",
        "        self,\n",
        "        prompts: List[str],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[Any] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> LLMResult:\n",
        "        \"\"\"Call the LLM on a list of prompts.\"\"\"\n",
        "        generations = []\n",
        "        for prompt in prompts:\n",
        "            # Pass **kwargs through to _call\n",
        "            text = self._call(prompt, stop, run_manager, **kwargs)\n",
        "            generations.append([{\"text\": text}]) # Wrap the result in the expected structure\n",
        "        return LLMResult(generations=generations)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "l7CiGSLpBL8h"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- 2. LangGraph AGENT DEFINITIONS ---\n",
        "\n",
        "# 2.1. Define the State for the Graph (Includes the input record data)\n",
        "class AgentState(TypedDict):\n",
        "    \"\"\"\n",
        "    The state of the graph, holding data passed between nodes.\n",
        "    \"\"\"\n",
        "    item_name: str\n",
        "    complexity_level: str\n",
        "    user_prompt: str\n",
        "    score: Optional[float]\n",
        "    review_data: Optional[str]\n",
        "\n",
        "\n",
        "# --- JSON Schema Definitions (Mandatory for Gemini JSON mode) ---\n",
        "\n",
        "# Schema for Agent 1: Score\n",
        "SCORE_SCHEMA = {\n",
        "    \"type\": \"OBJECT\",\n",
        "    \"properties\": {\n",
        "        \"score\": {\"type\": \"NUMBER\", \"description\": \"A random technical score between 0.0 and 1.0.\"},\n",
        "    },\n",
        "    \"required\": [\"score\"],\n",
        "    \"propertyOrdering\": [\"score\"]\n",
        "}\n",
        "\n",
        "# Schema for Agent 2: Detailed Review\n",
        "REVIEW_SCHEMA = {\n",
        "    \"type\": \"OBJECT\",\n",
        "    \"properties\": {\n",
        "        \"review_text\": {\"type\": \"STRING\", \"description\": \"A concise, technical review.\"},\n",
        "        \"category\": {\"type\": \"STRING\", \"description\": \"The category of the review (e.g., 'Positive', 'Neutral', 'Negative').\"}\n",
        "    },\n",
        "    \"required\": [\"review_text\", \"category\"],\n",
        "    \"propertyOrdering\": [\"review_text\", \"category\"]\n",
        "}\n",
        "\n",
        "\n",
        "# 2.2. Node for Agent 1: Score Generator\n",
        "def score_generator_node(state: AgentState) -> dict:\n",
        "    \"\"\"\n",
        "    Generates a technical score (0.0 to 1.0) for the item.\n",
        "    \"\"\"\n",
        "    print(\"--- [Agent 1] Executing: Score Generator ---\")\n",
        "\n",
        "    # 1. Initialize LLM with Score Schema\n",
        "    llm_score_generator = CustomHTTPGemini(model_name=\"gemini-2.5-flash\", response_schema=SCORE_SCHEMA)\n",
        "\n",
        "    # 2. Construct Prompt using the variables from the State\n",
        "    prompt_1 = PromptTemplate.from_template(\n",
        "        \"You are a technical analyst. Your task is to assign a random score between 0.0 and 1.0 to the '{item_name}' based on its complexity '{complexity_level}'. Use the following user instruction as context: '{user_prompt}'. Output the result strictly in JSON format according to the schema.\"\n",
        "    )\n",
        "    prompt_value = prompt_1.format(\n",
        "        item_name=state['item_name'],\n",
        "        complexity_level=state['complexity_level'],\n",
        "        user_prompt=state['user_prompt']\n",
        "    )\n",
        "\n",
        "    # Prepare base request data structure to comply with the mandatory requirement\n",
        "    base_request_data = {\n",
        "        \"contents\": [{\"parts\": [{\"text\": \"\"}]}] # Placeholder for prompt injection\n",
        "    }\n",
        "\n",
        "    # 3. Invoke LLM: Using _call instead of invoke, passing the base payload\n",
        "    raw_json_output = llm_score_generator._call(\n",
        "        prompt_value,\n",
        "        request_data=base_request_data\n",
        "    )\n",
        "\n",
        "    # 4. Parse JSON and update state\n",
        "    try:\n",
        "        score_data = json.loads(raw_json_output)\n",
        "        score = score_data.get('score', 0.0)\n",
        "        print(f\"--- [Agent 1] Generated Score: {score} ---\")\n",
        "        return {\"score\": score} # Return the update to the state\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON from Agent 1: {e}. Falling back to score 0.5.\")\n",
        "        return {\"score\": 0.5}\n",
        "\n",
        "\n",
        "# 2.3. Node for Agent 2: Review Generator\n",
        "def review_generator_node(state: AgentState) -> dict:\n",
        "    \"\"\"\n",
        "    Generates a detailed review based on the generated score and context.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- [Agent 2] Executing: Review Generator ---\")\n",
        "\n",
        "    # 1. Initialize LLM with Review Schema\n",
        "    llm_review_generator = CustomHTTPGemini(model_name=\"gemini-2.5-flash\", response_schema=REVIEW_SCHEMA)\n",
        "\n",
        "    # 2. Construct Prompt (uses 'score' and 'user_prompt' from the state)\n",
        "    prompt_2 = PromptTemplate.from_template(\n",
        "        \"Generate a technical review for the item '{item_name}' which has a complexity of '{complexity_level}' and received a technical score of {score}. Your review must reflect this score and adhere to the original instruction: '{user_prompt}'. Output the review strictly in JSON format according to the schema.\"\n",
        "    )\n",
        "\n",
        "    prompt_value = prompt_2.format(\n",
        "        item_name=state['item_name'],\n",
        "        complexity_level=state['complexity_level'],\n",
        "        score=state['score'],\n",
        "        user_prompt=state['user_prompt']\n",
        "    )\n",
        "\n",
        "    # Prepare base request data structure to comply with the mandatory requirement\n",
        "    base_request_data = {\n",
        "        \"contents\": [{\"parts\": [{\"text\": \"\"}]}] # Placeholder for prompt injection\n",
        "    }\n",
        "\n",
        "    # 3. Invoke LLM: Using _call instead of invoke, passing the base payload\n",
        "    raw_json_output = llm_review_generator._call(\n",
        "        prompt_value,\n",
        "        request_data=base_request_data\n",
        "    )\n",
        "\n",
        "    # 4. Update state with the final review data (raw JSON string)\n",
        "    print(\"--- [Agent 2] Generated Review Data ---\")\n",
        "    return {\"review_data\": raw_json_output}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMXTQF_gBQtB",
        "outputId": "15c49235-aa2f-4415-9dd1-1428f070018f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- LangGraph Custom HTTP Gemini BATCH PROCESSING Example ---\n",
            "\n",
            "Starting batch processing of CSV records...\n",
            "\n",
            "========================================================\n",
            "  PROCESSING RECORD 1/3: AI Debugging Assistant v3.0\n",
            "========================================================\n",
            "--- [Agent 1] Executing: Score Generator ---\n",
            "\n",
            "--- LLM Invoked for prompt (truncated): You are a technical analyst. Your task is to assig... ---\n",
            "Using custom request payload from **kwargs, injecting prompt.\n",
            "Applying JSON schema for structured output.\n",
            "--- [Agent 1] Generated Score: 0.7325 ---\n",
            "\n",
            "--- [Agent 2] Executing: Review Generator ---\n",
            "\n",
            "--- LLM Invoked for prompt (truncated): Generate a technical review for the item 'AI Debug... ---\n",
            "Using custom request payload from **kwargs, injecting prompt.\n",
            "Applying JSON schema for structured output.\n",
            "--- [Agent 2] Generated Review Data ---\n",
            "\n",
            "========================================================\n",
            "  PROCESSING RECORD 2/3: Basic E-commerce Frontend\n",
            "========================================================\n",
            "--- [Agent 1] Executing: Score Generator ---\n",
            "\n",
            "--- LLM Invoked for prompt (truncated): You are a technical analyst. Your task is to assig... ---\n",
            "Using custom request payload from **kwargs, injecting prompt.\n",
            "Applying JSON schema for structured output.\n",
            "--- [Agent 1] Generated Score: 0.35 ---\n",
            "\n",
            "--- [Agent 2] Executing: Review Generator ---\n",
            "\n",
            "--- LLM Invoked for prompt (truncated): Generate a technical review for the item 'Basic E-... ---\n",
            "Using custom request payload from **kwargs, injecting prompt.\n",
            "Applying JSON schema for structured output.\n",
            "--- [Agent 2] Generated Review Data ---\n",
            "\n",
            "========================================================\n",
            "  PROCESSING RECORD 3/3: Neural Network Optimizer v2\n",
            "========================================================\n",
            "--- [Agent 1] Executing: Score Generator ---\n",
            "\n",
            "--- LLM Invoked for prompt (truncated): You are a technical analyst. Your task is to assig... ---\n",
            "Using custom request payload from **kwargs, injecting prompt.\n",
            "Applying JSON schema for structured output.\n",
            "--- [Agent 1] Generated Score: 0.732 ---\n",
            "\n",
            "--- [Agent 2] Executing: Review Generator ---\n",
            "\n",
            "--- LLM Invoked for prompt (truncated): Generate a technical review for the item 'Neural N... ---\n",
            "Using custom request payload from **kwargs, injecting prompt.\n",
            "Applying JSON schema for structured output.\n",
            "--- [Agent 2] Generated Review Data ---\n",
            "\n",
            "========================================================\n",
            "ALL BATCH PROCESSING COMPLETE. CONSOLIDATED RESULTS:\n",
            "========================================================\n",
            "\n",
            "Record ID: 1 (AI Debugging Assistant v3.0)\n",
            "  Complexity: High/Production\n",
            "  Score: 0.73 | Category: Negative\n",
            "  Review: AI Debugging Assistant v3.0, despite its 0.7325 technical score, exhibits significant architectural shortcomings for high-complexity production environments. Diagnostic accuracy remains suboptimal in polyglot and distributed systems, leading to an elevated false positive rate that necessitates extensive manual verification. Integration complexity is high for non-standardized stacks, increasing deployment friction. Furthermore, the explainability of its AI-driven recommendations is often opaque, hindering developer trust and efficient remediation. Scalability for large-scale microservices requires substantial optimization to prevent performance bottlenecks.\n",
            "\n",
            "Record ID: 2 (Basic E-commerce Frontend)\n",
            "  Complexity: Low/Simple\n",
            "  Score: 0.35 | Category: Negative\n",
            "  Review: The 'Basic E-commerce Frontend' exhibits a fundamentally rudimentary technical implementation. It lacks modern architectural patterns, sophisticated state management, and performance optimizations essential for even moderate scalability. The simplistic design implies significant maintainability challenges and a highly constrained feature set, rendering it technically inadequate for competitive e-commerce deployments.\n",
            "\n",
            "Record ID: 3 (Neural Network Optimizer v2)\n",
            "  Complexity: Medium/Advanced\n",
            "  Score: 0.73 | Category: Neutral\n",
            "  Review: Neural Network Optimizer v2, despite its 'Advanced' complexity claim, exhibits a merely conventional technical foundation. Its 0.732 score reflects functional implementation of standard optimization techniques, yet critically lacks innovation in mitigating common convergence pathologies for complex architectures. The absence of adaptive learning rate schedules beyond rudimentary methods and limited support for distributed training paradigms significantly caps its market potential in high-performance environments. Robustness against hyperparameter sensitivity is also underwhelming, demanding extensive manual tuning.\n",
            "\n",
            "--- Raw JSON List of All Results ---\n",
            "[\n",
            "  {\n",
            "    \"record_id\": 1,\n",
            "    \"item_name\": \"AI Debugging Assistant v3.0\",\n",
            "    \"complexity_level\": \"High/Production\",\n",
            "    \"generated_score\": 0.7325,\n",
            "    \"review_category\": \"Negative\",\n",
            "    \"review_text\": \"AI Debugging Assistant v3.0, despite its 0.7325 technical score, exhibits significant architectural shortcomings for high-complexity production environments. Diagnostic accuracy remains suboptimal in polyglot and distributed systems, leading to an elevated false positive rate that necessitates extensive manual verification. Integration complexity is high for non-standardized stacks, increasing deployment friction. Furthermore, the explainability of its AI-driven recommendations is often opaque, hindering developer trust and efficient remediation. Scalability for large-scale microservices requires substantial optimization to prevent performance bottlenecks.\"\n",
            "  },\n",
            "  {\n",
            "    \"record_id\": 2,\n",
            "    \"item_name\": \"Basic E-commerce Frontend\",\n",
            "    \"complexity_level\": \"Low/Simple\",\n",
            "    \"generated_score\": 0.35,\n",
            "    \"review_category\": \"Negative\",\n",
            "    \"review_text\": \"The 'Basic E-commerce Frontend' exhibits a fundamentally rudimentary technical implementation. It lacks modern architectural patterns, sophisticated state management, and performance optimizations essential for even moderate scalability. The simplistic design implies significant maintainability challenges and a highly constrained feature set, rendering it technically inadequate for competitive e-commerce deployments.\"\n",
            "  },\n",
            "  {\n",
            "    \"record_id\": 3,\n",
            "    \"item_name\": \"Neural Network Optimizer v2\",\n",
            "    \"complexity_level\": \"Medium/Advanced\",\n",
            "    \"generated_score\": 0.732,\n",
            "    \"review_category\": \"Neutral\",\n",
            "    \"review_text\": \"Neural Network Optimizer v2, despite its 'Advanced' complexity claim, exhibits a merely conventional technical foundation. Its 0.732 score reflects functional implementation of standard optimization techniques, yet critically lacks innovation in mitigating common convergence pathologies for complex architectures. The absence of adaptive learning rate schedules beyond rudimentary methods and limited support for distributed training paradigms significantly caps its market potential in high-performance environments. Robustness against hyperparameter sensitivity is also underwhelming, demanding extensive manual tuning.\"\n",
            "  }\n",
            "]\n",
            "\n",
            "--- End of Batch Execution ---\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# --- 2.4. LangGraph Setup and Execution (Batch Processing) ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Setup ---\n",
        "    print(\"--- LangGraph Custom HTTP Gemini BATCH PROCESSING Example ---\")\n",
        "\n",
        "    # NOTE: Set your API Key in your environment before running:\n",
        "    # export GEMINI_API_KEY=\"YOUR_API_KEY_HERE\"\n",
        "\n",
        "    # Define the generic user prompt that applies to all records\n",
        "    GENERAL_USER_PROMPT = \"Analyze the item's technical sophistication and market potential, focusing only on the technical aspects and giving brief, highly critical feedback.\"\n",
        "\n",
        "    # Ensure API Key is available before starting the graph\n",
        "    if not os.getenv(\"GEMINI_API_KEY\"):\n",
        "        print(\"\\nERROR: GEMINI_API_KEY environment variable not set.\")\n",
        "        print(\"Please set the GEMINI_API_KEY environment variable and try again.\")\n",
        "        exit()\n",
        "\n",
        "    # --- Build the Graph ---\n",
        "    graph_builder = StateGraph(AgentState)\n",
        "\n",
        "    # Add the nodes (agents) to the graph\n",
        "    graph_builder.add_node(\"score_generator\", score_generator_node)\n",
        "    graph_builder.add_node(\"review_generator\", review_generator_node)\n",
        "\n",
        "    # Set the entry point\n",
        "    graph_builder.set_entry_point(\"score_generator\")\n",
        "\n",
        "    # Define the sequence: Score Generator -> Review Generator -> END\n",
        "    graph_builder.add_edge(\"score_generator\", \"review_generator\")\n",
        "    graph_builder.add_edge(\"review_generator\", END)\n",
        "\n",
        "    # Compile the graph into a runnable application\n",
        "    app = graph_builder.compile()\n",
        "\n",
        "    # --- CSV Simulation for Batch Processing ---\n",
        "    # This list simulates records read from a CSV file\n",
        "    csv_records = [\n",
        "        {\"item_name\": \"AI Debugging Assistant v3.0\", \"complexity_level\": \"High/Production\"},\n",
        "        {\"item_name\": \"Basic E-commerce Frontend\", \"complexity_level\": \"Low/Simple\"},\n",
        "        {\"item_name\": \"Neural Network Optimizer v2\", \"complexity_level\": \"Medium/Advanced\"},\n",
        "    ]\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    print(\"\\nStarting batch processing of CSV records...\")\n",
        "\n",
        "    for i, record in enumerate(csv_records):\n",
        "        print(f\"\\n========================================================\")\n",
        "        print(f\"  PROCESSING RECORD {i + 1}/{len(csv_records)}: {record['item_name']}\")\n",
        "        print(f\"========================================================\")\n",
        "\n",
        "        # 1. Prepare the initial state for the current record\n",
        "        # LangGraph requires a single state dictionary input.\n",
        "        initial_state = {\n",
        "            \"item_name\": record[\"item_name\"],\n",
        "            \"complexity_level\": record[\"complexity_level\"],\n",
        "            \"user_prompt\": GENERAL_USER_PROMPT\n",
        "        }\n",
        "\n",
        "        # 2. Invoke the graph for the current record\n",
        "        final_state = app.invoke(initial_state)\n",
        "\n",
        "        final_review_json = final_state.get('review_data')\n",
        "        final_score = final_state.get('score')\n",
        "\n",
        "        # 3. Process and consolidate results\n",
        "        try:\n",
        "            # Attempt to parse the final JSON review data\n",
        "            parsed_review = json.loads(final_review_json)\n",
        "        except json.JSONDecodeError:\n",
        "            parsed_review = {\"review_text\": \"JSON decode failed\", \"category\": \"Error\"}\n",
        "\n",
        "        result = {\n",
        "            \"record_id\": i + 1,\n",
        "            \"item_name\": record[\"item_name\"],\n",
        "            \"complexity_level\": record[\"complexity_level\"],\n",
        "            \"generated_score\": final_score,\n",
        "            \"review_category\": parsed_review.get('category', 'N/A'),\n",
        "            \"review_text\": parsed_review.get('review_text', 'N/A')\n",
        "        }\n",
        "        all_results.append(result)\n",
        "\n",
        "    print(\"\\n========================================================\")\n",
        "    print(\"ALL BATCH PROCESSING COMPLETE. CONSOLIDATED RESULTS:\")\n",
        "    print(\"========================================================\")\n",
        "\n",
        "    for result in all_results:\n",
        "        print(f\"\\nRecord ID: {result['record_id']} ({result['item_name']})\")\n",
        "        print(f\"  Complexity: {result['complexity_level']}\")\n",
        "        print(f\"  Score: {result['generated_score']:.2f} | Category: {result['review_category']}\")\n",
        "        print(f\"  Review: {result['review_text']}\")\n",
        "\n",
        "    print(\"\\n--- Raw JSON List of All Results ---\")\n",
        "    print(json.dumps(all_results, indent=2))\n",
        "    print(\"\\n--- End of Batch Execution ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hKqm-AjBlL_"
      },
      "outputs": [],
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Setup ---\n",
        "    print(\"--- LangGraph Custom HTTP Gemini BATCH PROCESSING Example ---\")\n",
        "    \n",
        "    # NOTE: Set your API Key in your environment before running:\n",
        "    # export GEMINI_API_KEY=\"YOUR_API_KEY_HERE\"\n",
        "    \n",
        "    # Define the generic user prompt that applies to all records\n",
        "    GENERAL_USER_PROMPT = \"Analyze the item's technical sophistication and market potential, focusing only on the technical aspects and giving brief, highly critical feedback.\"\n",
        "\n",
        "    # Ensure API Key is available before starting the graph\n",
        "    if not os.getenv(\"GEMINI_API_KEY\"):\n",
        "        print(\"\\nERROR: GEMINI_API_KEY environment variable not set.\")\n",
        "        print(\"Please set the GEMINI_API_KEY environment variable and try again.\")\n",
        "        exit()\n",
        "\n",
        "    # --- Build the Graph ---\n",
        "    graph_builder = StateGraph(AgentState)\n",
        "\n",
        "    # Add the nodes (agents) to the graph\n",
        "    graph_builder.add_node(\"score_generator\", score_generator_node)\n",
        "    graph_builder.add_node(\"review_generator\", review_generator_node) # Node added but unlinked\n",
        "\n",
        "    # Set the entry point\n",
        "    graph_builder.set_entry_point(\"score_generator\")\n",
        "\n",
        "    # MODIFIED: Define the sequence: Score Generator -> END\n",
        "    # The review_generator is now unlinked from the main flow\n",
        "    graph_builder.add_edge(\"score_generator\", END)\n",
        "\n",
        "    # Compile the graph into a runnable application\n",
        "    app = graph_builder.compile()\n",
        "    \n",
        "    # --- CSV Simulation for Batch Processing ---\n",
        "    # This list simulates records read from a CSV file\n",
        "    csv_records = [\n",
        "        {\"item_name\": \"AI Debugging Assistant v3.0\", \"complexity_level\": \"High/Production\"},\n",
        "        {\"item_name\": \"Basic E-commerce Frontend\", \"complexity_level\": \"Low/Simple\"},\n",
        "        {\"item_name\": \"Neural Network Optimizer v2\", \"complexity_level\": \"Medium/Advanced\"},\n",
        "    ]\n",
        "    \n",
        "    all_results = []\n",
        "    \n",
        "    print(\"\\nStarting batch processing of CSV records (Running ONLY Agent 1: Score Generator)...\")\n",
        "    \n",
        "    for i, record in enumerate(csv_records):\n",
        "        print(f\"\\n========================================================\")\n",
        "        print(f\"  PROCESSING RECORD {i + 1}/{len(csv_records)}: {record['item_name']}\")\n",
        "        print(f\"========================================================\")\n",
        "        \n",
        "        # 1. Prepare the initial state for the current record\n",
        "        initial_state = {\n",
        "            \"item_name\": record[\"item_name\"],\n",
        "            \"complexity_level\": record[\"complexity_level\"],\n",
        "            \"user_prompt\": GENERAL_USER_PROMPT \n",
        "        }\n",
        "\n",
        "        # 2. Invoke the graph for the current record\n",
        "        # This run will stop immediately after score_generator_node completes.\n",
        "        final_state = app.invoke(initial_state)\n",
        "\n",
        "        final_score = final_state.get('score')\n",
        "        \n",
        "        # 3. Process and consolidate results\n",
        "        result = {\n",
        "            \"record_id\": i + 1,\n",
        "            \"item_name\": record[\"item_name\"],\n",
        "            \"complexity_level\": record[\"complexity_level\"],\n",
        "            \"generated_score\": final_score,\n",
        "            \"review_category\": \"NOT RUN\", # Indicate that Agent 2 did not run\n",
        "            \"review_text\": \"Agent 2 (Review Generator) was bypassed.\" \n",
        "        }\n",
        "        all_results.append(result)\n",
        "        \n",
        "    print(\"\\n========================================================\")\n",
        "    print(\"BATCH PROCESSING COMPLETE. RESULTS FROM AGENT 1 ONLY:\")\n",
        "    print(\"========================================================\")\n",
        "    \n",
        "    for result in all_results:\n",
        "        print(f\"\\nRecord ID: {result['record_id']} ({result['item_name']})\")\n",
        "        print(f\"  Complexity: {result['complexity_level']}\")\n",
        "        print(f\"  Score: {result['generated_score']:.2f} | Review Status: {result['review_category']}\")\n",
        "        \n",
        "    print(\"\\n--- Raw JSON List of All Results (Agent 2 data is absent) ---\")\n",
        "    # Clean up results for final output to only show relevant data\n",
        "    clean_results = [{k: v for k, v in res.items() if k not in ['review_category', 'review_text']} for res in all_results]\n",
        "    print(json.dumps(clean_results, indent=2))\n",
        "    print(\"\\n--- End of Batch Execution ---\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
